# ğŸ“Š ÙØ±Ø¢ÛŒÙ†Ø¯ Ú©Ø§Ù…Ù„ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® RAG

## ğŸ¯ Ù†Ù…ÙˆØ¯Ø§Ø± Ú©Ù„ÛŒ ÙØ±Ø¢ÛŒÙ†Ø¯

```
Ø³ÛŒØ³ØªÙ… Ú©Ø§Ø±Ø¨Ø±Ø§Ù† â†’ API Gateway â†’ Core System â†’ RAG Pipeline â†’ LLMs â†’ Qdrant â†’ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ â†’ Ø³ÛŒØ³ØªÙ… Ú©Ø§Ø±Ø¨Ø±Ø§Ù†
```

---

## ğŸ“ Ù…Ø±Ø­Ù„Ù‡ 1: Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ø² Ø³ÛŒØ³ØªÙ… Ú©Ø§Ø±Ø¨Ø±Ø§Ù†

### ÙØ§ÛŒÙ„: `/srv/app/api/v1/endpoints/query.py` (Ø®Ø· 219-288)

```python
@router.post("/")
async def process_query(
    request: QueryRequest,  # Ø´Ø§Ù…Ù„: query, language, max_results, filters, ...
    user_id: str = Depends(get_current_user_id)  # JWT authentication
)
```

**ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² Ø³ÛŒØ³ØªÙ… Ú©Ø§Ø±Ø¨Ø±Ø§Ù†:**
```json
{
  "query": "Ù…Ø§Ø¯Ù‡ Ø¯Ù‡ Ù‚Ø§Ù†ÙˆÙ† Ú†Ù„Ù…Ù†Ú¯Ø§Ù† Ú†ÛŒ Ù…ÛŒ Ú¯Ù‡ØŸ",
  "language": "fa",
  "max_results": 5,
  "use_cache": true,
  "use_reranking": true,
  "filters": null
}
```

**Ú©Ø§Ø±Ù‡Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡:**
1. âœ… **Authentication**: Ø¨Ø±Ø±Ø³ÛŒ JWT token Ùˆ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø§Ø±Ø¨Ø±
2. âœ… **Ø¨Ø±Ø±Ø³ÛŒ User Profile**: 
   - Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø³Øª â†’ Ø§ÛŒØ¬Ø§Ø¯ profile
   - Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø±ÙˆØ²Ø§Ù†Ù‡ (daily query limit)
3. âœ… **Ù…Ø¯ÛŒØ±ÛŒØª Conversation**:
   - Ø§Ú¯Ø± conversation_id Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ â†’ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ
   - Ø§Ú¯Ø± Ù†Ù‡ â†’ Ø§ÛŒØ¬Ø§Ø¯ conversation Ø¬Ø¯ÛŒØ¯
4. âœ… **Ø³Ø§Ø®Øª RAGQuery object**:
```python
rag_query = RAGQuery(
    text=request.query,
    user_id=str(user.id),
    conversation_id=str(conversation.id),
    language=request.language,
    max_chunks=request.max_results,
    filters=request.filters,
    use_cache=request.use_cache,
    use_reranking=request.use_reranking,
    user_preferences=request.user_preferences
)
```

---

## ğŸ“ Ù…Ø±Ø­Ù„Ù‡ 2: ÙˆØ±ÙˆØ¯ Ø¨Ù‡ RAG Pipeline

### ÙØ§ÛŒÙ„: `/srv/app/rag/pipeline.py` - Ù…ØªØ¯ `process()` (Ø®Ø· 75-179)

```python
pipeline = RAGPipeline()
rag_response = await pipeline.process(rag_query)
```

### ğŸ”¹ Step 0: Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø³ÙˆØ§Ù„ Ø¨Ø§ LLM (Ø®Ø· 88-109)

**Ù‡Ø¯Ù:** ØªØ´Ø®ÛŒØµ Ø§ÛŒÙ†Ú©Ù‡ Ø³ÙˆØ§Ù„ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³Øª ÛŒØ§ Ø§Ø­ÙˆØ§Ù„Ù¾Ø±Ø³ÛŒ/Ú†Ø±Øªâ€ŒÙˆÙ¾Ø±Øª

```python
classification = await self.classifier.classify(query.text, query.language)
```

#### ÙØ§ÛŒÙ„: `/srv/app/llm/classifier.py` (Ø®Ø· 45-86)

**Prompt Ø§Ø±Ø³Ø§Ù„ÛŒ Ø¨Ù‡ LLM:**
```python
system_prompt = """Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù†Ù†Ø¯Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³ÙˆØ§Ù„Ø§Øª Ù‡Ø³ØªÛŒØ¯.

Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§:
1. greeting - Ø§Ø­ÙˆØ§Ù„Ù¾Ø±Ø³ÛŒ
2. chitchat - Ú¯ÙØªÚ¯ÙˆÛŒ Ø¹Ù…ÙˆÙ…ÛŒ
3. invalid - Ù…Ø­ØªÙˆØ§ÛŒ Ù†Ø§Ù…Ø¹ØªØ¨Ø±
4. business_question - Ø³ÙˆØ§Ù„ ÙˆØ§Ù‚Ø¹ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ú©Ø³Ø¨ Ùˆ Ú©Ø§Ø±/Ù‚Ø§Ù†ÙˆÙ†

Ø®Ø±ÙˆØ¬ÛŒ JSON:
{
  "category": "...",
  "confidence": 0.0-1.0,
  "direct_response": "...",
  "reason": "..."
}
"""

user_message = f"Ù…ØªÙ† Ú©Ø§Ø±Ø¨Ø±: {query}"
```

**ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ LLM:**
```python
# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² gpt-4o-mini Ø¨Ø§ temperature=0.2
response = await self.llm.generate(messages)
```

**Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù¾Ø§Ø³Ø®:**
```python
result = self._parse_classification_response(response.content)

if result.category != "business_question":
    # Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø¯ÙˆÙ† RAG
    return RAGResponse(
        answer=result.direct_response,
        chunks=[],
        sources=[],
        ...
    )
# Ø§Ú¯Ø± business_question Ø¨ÙˆØ¯ â†’ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ù‡ RAG
```

---

### ğŸ”¹ Step 1: Ø¨Ø±Ø±Ø³ÛŒ Cache (Ø®Ø· 114-118)

```python
if query.use_cache:
    cached_response = await self._check_cache(query)
    if cached_response:
        return cached_response  # Ø¨Ø§Ø²Ú¯Ø´Øª Ø³Ø±ÛŒØ¹
```

**Cache Key:**
```python
key = md5(f"{query.text}|{language}|{max_chunks}|{filters}")
# Ù…Ø«Ø§Ù„: "rag:cache:a3f5d8e9..."
```

---

### ğŸ”¹ Step 2: Ø¨Ù‡Ø¨ÙˆØ¯ Query Ø¨Ø§ LLM (Ø®Ø· 121)

```python
enhanced_query = await self._enhance_query(query)
```

#### ÙØ§ÛŒÙ„: `/srv/app/rag/pipeline.py` - Ù…ØªØ¯ `_enhance_query()` (Ø®Ø· 204-273)

**Prompt Ø§Ø±Ø³Ø§Ù„ÛŒ Ø¨Ù‡ LLM:**
```python
system_prompt = """Ø´Ù…Ø§ ÛŒÚ© Ù…ØªØ®ØµØµ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø³Ù†Ø§Ø¯ Ø­Ù‚ÙˆÙ‚ÛŒ Ù‡Ø³ØªÛŒØ¯.

Ú©Ø§Ø±Ù‡Ø§ÛŒ Ø´Ù…Ø§:
1. Ø§Ø®ØªØµØ§Ø±Ø§Øª Ù‚ÙˆØ§Ù†ÛŒÙ† Ø±Ø§ Ø¨Ø§Ø² Ú©Ù†ÛŒØ¯ (Ù‚.Ù… â†’ Ù‚Ø§Ù†ÙˆÙ† Ù…Ø¯Ù†ÛŒ)
2. Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯ (Û±Û²Û³ â†’ 123)
3. Ø§Ø¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ù…ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯ (Ø¯Ù‡ â†’ 10)
4. Ø§Ù…Ù„Ø§ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡ Ø±Ø§ ØªØµØ­ÛŒØ­ Ú©Ù†ÛŒØ¯
5. Ú©Ù„Ù…Ø§Øª Ù…ØªØ±Ø§Ø¯Ù Ù…Ù‡Ù… Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯

Ù…Ø«Ø§Ù„:
ÙˆØ±ÙˆØ¯ÛŒ: "Ù…Ø§Ø¯Ù‡ Ø¯Ù‡ Ù‚Ø§Ù†ÙˆÙ† Ú†Ù„Ù…Ù†Ú¯Ø§Ù†"
Ø®Ø±ÙˆØ¬ÛŒ: "Ù…Ø§Ø¯Ù‡ 10 Ù‚Ø§Ù†ÙˆÙ† Ú†Ù„Ù…Ù†Ú¯Ø§Ù†"

ÙÙ‚Ø· query Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯."""

user_message = f"Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {query.text}"
```

**ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ LLM:**
```python
response = await self.llm.generate(
    messages,
    temperature=0.1,  # Ú©Ù… Ø¨Ø±Ø§ÛŒ consistency
    max_tokens=200
)

enhanced = response.content.strip()
```

**Ù…Ø«Ø§Ù„:**
```
ÙˆØ±ÙˆØ¯ÛŒ: "Ù…Ø§Ø¯Ù‡ Ø¯Ù‡ Ù‚Ø§Ù†ÙˆÙ† Ú†Ù„Ù…Ù†Ú¯Ø§Ù† Ú†ÛŒ Ù…ÛŒ Ú¯Ù‡ØŸ"
â†“
Ø®Ø±ÙˆØ¬ÛŒ: "Ù…Ø§Ø¯Ù‡ 10 Ù‚Ø§Ù†ÙˆÙ† Ú†Ù„Ù…Ù†Ú¯Ø§Ù†"
```

---

### ğŸ”¹ Step 3: ØªÙˆÙ„ÛŒØ¯ Embedding (Ø®Ø· 124)

```python
query_embedding = await self._generate_embedding(enhanced_query)
```

#### ÙØ§ÛŒÙ„: `/srv/app/services/embedding_service.py` (Ø®Ø· 142-153)

```python
# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ local: intfloat/multilingual-e5-large
embedding = embedder.encode_single(text)  # numpy array [1024]
return embedding.tolist()  # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ list
```

**Ø®Ø±ÙˆØ¬ÛŒ:**
```python
query_embedding = [0.023, -0.145, 0.089, ..., 0.234]  # 1024 dimensions
```

---

### ğŸ”¹ Step 4: Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Qdrant (Ø®Ø· 127-132)

```python
chunks = await self._retrieve_chunks(
    query_embedding,
    enhanced_query,
    query.filters,
    limit=query.max_chunks * 3  # 5 * 3 = 15 chunks
)
```

#### ÙØ§ÛŒÙ„: `/srv/app/rag/pipeline.py` - Ù…ØªØ¯ `_retrieve_chunks()` (Ø®Ø· 274-349)

**ØªØ´Ø®ÛŒØµ Vector Field:**
```python
vector_field = self._get_vector_field(len(query_embedding))
# Ø¨Ø±Ø§ÛŒ 1024 dim â†’ "large"
```

**Ø§Ù†ØªØ®Ø§Ø¨ Ù†ÙˆØ¹ Ø¬Ø³ØªØ¬Ùˆ:**
```python
if settings.rag_use_hybrid_search:  # True
    results = await self.qdrant.hybrid_search(...)
else:
    results = await self.qdrant.search(...)
```

#### ÙØ§ÛŒÙ„: `/srv/app/services/qdrant_service.py` - Ù…ØªØ¯ `hybrid_search()` (Ø®Ø· 319-371)

**âš ï¸ ØªÙˆØ¬Ù‡:** Ø¨Ø¹Ø¯ Ø§Ø² Ø±ÙØ¹ bugØŒ hybrid search ÙØ¹Ù„Ø§Ù‹ ÙÙ‚Ø· vector search Ø§Ø³Øª:

```python
async def hybrid_search(...):
    # ÙÙ‚Ø· vector search Ø¨Ø§ threshold Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±
    vector_results = await self.search(
        query_vector=query_vector,
        limit=limit,
        score_threshold=0.4,  # Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ recall Ø¨Ù‡ØªØ±
        filters=filters,
        vector_field="large"
    )
    return vector_results
```

#### ÙØ§ÛŒÙ„: `/srv/app/services/qdrant_service.py` - Ù…ØªØ¯ `search()` (Ø®Ø· 242-317)

**Ø³Ø§Ø®Øª Filters:**
```python
filter_conditions = []
if filters:
    for key, value in filters.items():
        filter_conditions.append(
            FieldCondition(key=key, match=MatchValue(value=value))
        )

search_filter = Filter(must=filter_conditions) if filter_conditions else None
```

**ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Qdrant:**
```python
results = self.client.search(
    collection_name="legal_documents",
    query_vector=("large", query_embedding),  # Named vector
    limit=15,
    score_threshold=0.4,
    query_filter=search_filter,
    with_payload=True
)
```

**Ù¾Ø§Ø³Ø® Qdrant:**
```python
[
    {
        "id": "abc123",
        "score": 0.87,
        "payload": {
            "text": "Ù…Ø§Ø¯Ù‡ 10 - Ù…ØªÙ† Ù…Ø§Ø¯Ù‡...",
            "document_id": "doc_456",
            "metadata": {
                "work_title": "Ù‚Ø§Ù†ÙˆÙ† Ú†Ù„Ù…Ù†Ú¯Ø§Ù†",
                "unit_number": "10",
                "unit_type": "article",
                ...
            }
        }
    },
    ...
]
```

**ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ RAGChunk:**
```python
chunks = []
for result in results:
    chunk = RAGChunk(
        text=result["text"],
        score=result.get("score", 0.0),
        source=result.get("source", "unknown"),
        metadata=result.get("metadata", {}),
        document_id=result.get("document_id")
    )
    chunks.append(chunk)
```

**Ù„Ø§Ú¯:**
```python
logger.info(
    "Retrieved chunks",
    num_chunks=len(chunks),  # Ù…Ø«Ù„Ø§Ù‹ 12
    top_scores=[0.87, 0.82, 0.79]
)
```

---

### ğŸ”¹ Step 5: Reranking (Ø®Ø· 143-159)

```python
if query.use_reranking and len(chunks) > query.max_chunks:
    chunks = await self._rerank_chunks(
        enhanced_query,
        chunks,
        top_k=query.max_chunks  # 5
    )
else:
    chunks = chunks[:query.max_chunks]
```

#### ÙØ§ÛŒÙ„: `/srv/app/rag/pipeline.py` - Ù…ØªØ¯ `_rerank_chunks()` (Ø®Ø· 351-385)

**âš ï¸ ØªÙˆØ¬Ù‡:** Cohere reranker ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª (API key Ø®Ø§Ù„ÛŒ)

```python
if settings.cohere_api_key and self.reranker:
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Cohere reranker
    ...
else:
    # Fallback: Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ score
    return sorted(chunks, key=lambda x: x.score, reverse=True)[:top_k]
```

**Ø®Ø±ÙˆØ¬ÛŒ:**
```python
# 5 chunk Ø¨Ø±ØªØ±
chunks = [chunk1, chunk2, chunk3, chunk4, chunk5]
```

---

### ğŸ”¹ Step 6: ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ LLM (Ø®Ø· 168-174)

```python
answer, tokens_used = await self._generate_answer(
    query.text,
    chunks,
    query.language,
    query.conversation_id,
    query.user_preferences
)
```

#### ÙØ§ÛŒÙ„: `/srv/app/rag/pipeline.py` - Ù…ØªØ¯ `_generate_answer()` (Ø®Ø· 387-457)

**Ø³Ø§Ø®Øª Context:**
```python
context_parts = []
for i, chunk in enumerate(chunks, 1):
    source_info = f"[Ù…Ù†Ø¨Ø¹ {i}]"
    work_title = chunk.metadata.get("work_title")
    if work_title:
        source_info += f" {work_title}"
    if chunk.metadata.get("unit_number"):
        source_info += f" - Ù…Ø§Ø¯Ù‡ {chunk.metadata['unit_number']}"
    
    context_parts.append(f"{source_info}:\n{chunk.text}")

context = "\n\n".join(context_parts)
```

**Ù…Ø«Ø§Ù„ Context:**
```
[Ù…Ù†Ø¨Ø¹ 1] Ù‚Ø§Ù†ÙˆÙ† Ú†Ù„Ù…Ù†Ú¯Ø§Ù† - Ù…Ø§Ø¯Ù‡ 10:
Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ù…Ø§Ø¯Ù‡ 10...

[Ù…Ù†Ø¨Ø¹ 2] Ù‚Ø§Ù†ÙˆÙ† Ú†Ù„Ù…Ù†Ú¯Ø§Ù† - Ù…Ø§Ø¯Ù‡ 11:
Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ù…Ø§Ø¯Ù‡ 11...

...
```

**Ø³Ø§Ø®Øª System Prompt:**
```python
system_prompt = """Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø³Ø¨ Ùˆ Ú©Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚ÙˆØ§Ù†ÛŒÙ† Ùˆ Ù…Ù‚Ø±Ø±Ø§Øª Ø§ÛŒØ±Ø§Ù† Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯.

ÙˆØ¸Ø§ÛŒÙ Ø´Ù…Ø§:
- Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø¬Ø§Ù…Ø¹ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±Ø¬Ø¹ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡
- Ø§Ø±Ø¬Ø§Ø¹ Ø¨Ù‡ Ù…Ù†Ø§Ø¨Ø¹ Ùˆ Ù…ÙˆØ§Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø±ØªØ¨Ø·
- ØªÙˆØ¶ÛŒØ­ Ù…ÙØ§Ù‡ÛŒÙ… Ø­Ù‚ÙˆÙ‚ÛŒ Ø¨Ù‡ Ø²Ø¨Ø§Ù† Ø³Ø§Ø¯Ù‡
- Ø§Ø´Ø§Ø±Ù‡ Ø¨Ù‡ Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ùˆ Ø§Ø³ØªØ«Ù†Ø§Ù‡Ø§

Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§:
- ÙÙ‚Ø· Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±Ø¬Ø¹ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
- Ø§Ø² Ø§Ø¸Ù‡Ø§Ø± Ù†Ø¸Ø± Ø´Ø®ØµÛŒ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯
- Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ù†Ø¯Ø§Ø±ÛŒØ¯ØŒ ØµØ±Ø§Ø­ØªØ§Ù‹ Ø§Ø¹Ù„Ø§Ù… Ú©Ù†ÛŒØ¯"""
```

**Ø³Ø§Ø®Øª User Message:**
```python
user_message = f"""Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {query.text}

Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±Ø¬Ø¹:
{context}"""
```

**ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ LLM:**
```python
messages = [
    Message(role="system", content=system_prompt),
    Message(role="user", content=user_message)
]

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² gpt-4o-mini
response = await self.llm.generate(messages)
```

#### ÙØ§ÛŒÙ„: `/srv/app/llm/openai_provider.py` - Ù…ØªØ¯ `generate()` (Ø®Ø· 52-99)

**Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ API Call:**
```python
params = {
    "model": "gpt-4o-mini",
    "messages": [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ],
    "max_tokens": 4096,
    "temperature": 0.4,
    "top_p": 1.0,
    "frequency_penalty": 0.0,
    "presence_penalty": 0.0
}

response = await self.client.chat.completions.create(**params)
```

**Ù¾Ø§Ø³Ø® LLM:**
```python
LLMResponse(
    content="Ø·Ø¨Ù‚ Ù…Ø§Ø¯Ù‡ 10 Ù‚Ø§Ù†ÙˆÙ† Ú†Ù„Ù…Ù†Ú¯Ø§Ù†ØŒ ...",
    model="gpt-4o-mini",
    usage={
        "prompt_tokens": 1250,
        "completion_tokens": 320,
        "total_tokens": 1570
    },
    finish_reason="stop"
)
```

---

### ğŸ”¹ Step 7: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù†Ø§Ø¨Ø¹ (Ø®Ø· 176-177)

```python
sources = self._extract_sources(chunks)
```

#### ÙØ§ÛŒÙ„: `/srv/app/rag/pipeline.py` - Ù…ØªØ¯ `_extract_sources()` (Ø®Ø· 588-658)

**ÙØ±Ù…Øª Ù…Ù†Ø§Ø¨Ø¹:**
```python
sources = []
for i, chunk in enumerate(chunks, 1):
    source_lines = [
        f"ğŸ“Œ Ù…Ù†Ø¨Ø¹ {i}:",
        f"ğŸ“„ Ù…ØªÙ†: {chunk.text}",
        "",
        f"ğŸ“• Ù†Ø§Ù… Ø³Ù†Ø¯: {work_title}",
        f"ğŸ“ Ù…Ø³ÛŒØ±: {path_label}",
        f"âœ… Ù…Ø±Ø¬Ø¹ ØªØµÙˆÛŒØ¨: {authority}"  # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ ØºÛŒØ± Ù‚ÙˆØ§Ù†ÛŒÙ†
    ]
    sources.append("\n".join(source_lines))
```

**Ù…Ø«Ø§Ù„ Ø®Ø±ÙˆØ¬ÛŒ:**
```
ğŸ“Œ Ù…Ù†Ø¨Ø¹ 1:
ğŸ“„ Ù…ØªÙ†: Ù…Ø§Ø¯Ù‡ 10 - Ù‡Ø± Ú©Ø³...
ğŸ“• Ù†Ø§Ù… Ø³Ù†Ø¯: Ù‚Ø§Ù†ÙˆÙ† Ú†Ù„Ù…Ù†Ú¯Ø§Ù†
ğŸ“ Ù…Ø³ÛŒØ±: ÙØµÙ„ 2 > Ø¨Ø®Ø´ 1 > Ù…Ø§Ø¯Ù‡ 10

ğŸ“Œ Ù…Ù†Ø¨Ø¹ 2:
...
```

---

### ğŸ”¹ Step 8: Ø³Ø§Ø®Øª Response Ùˆ Cache (Ø®Ø· 179-196)

```python
response = RAGResponse(
    answer=answer,
    chunks=chunks,
    sources=sources,
    total_tokens=tokens_used,
    processing_time_ms=processing_time,
    model_used=self.llm.config.model
)

if query.use_cache:
    await self._cache_response(query, response)

return response
```

---

## ğŸ“ Ù…Ø±Ø­Ù„Ù‡ 3: Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Database Ùˆ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±

### ÙØ§ÛŒÙ„: `/srv/app/api/v1/endpoints/query.py` (Ø®Ø· 290-361)

**Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§:**
```python
# Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±
user_message = DBMessage(
    id=uuid.uuid4(),
    conversation_id=conversation.id,
    role=MessageRole.USER,
    content=request.query,
    created_at=datetime.utcnow()
)
db.add(user_message)

# Ù¾ÛŒØ§Ù… Ø¯Ø³ØªÛŒØ§Ø±
assistant_message = DBMessage(
    id=uuid.uuid4(),
    conversation_id=conversation.id,
    role=MessageRole.ASSISTANT,
    content=rag_response.answer,
    tokens=rag_response.total_tokens,
    processing_time_ms=rag_response.processing_time_ms,
    retrieved_chunks=[...],  # Ø°Ø®ÛŒØ±Ù‡ chunks Ø¨Ø±Ø§ÛŒ debug
    sources=rag_response.sources,
    model_used=rag_response.model_used,
    created_at=datetime.utcnow()
)
db.add(assistant_message)
```

**Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±:**
```python
conversation.message_count += 2
conversation.total_tokens += rag_response.total_tokens
user.increment_query_count()
user.total_tokens_used += rag_response.total_tokens

await db.commit()
```

**Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Ø³ÛŒØ³ØªÙ… Ú©Ø§Ø±Ø¨Ø±Ø§Ù† (Background Task):**
```python
from app.tasks.notifications import send_query_result_to_users

send_query_result_to_users.delay(
    user_id=str(user.id),
    conversation_id=str(conversation.id),
    message_id=str(assistant_message.id),
    query=request.query,
    answer=rag_response.answer,
    sources=rag_response.sources,
    tokens_used=rag_response.total_tokens,
    processing_time_ms=rag_response.processing_time_ms
)
```

**Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ù‡ API:**
```python
return QueryResponse(
    answer=rag_response.answer,
    sources=rag_response.sources,
    conversation_id=str(conversation.id),
    message_id=str(assistant_message.id),
    tokens_used=rag_response.total_tokens,
    processing_time_ms=rag_response.processing_time_ms,
    cached=rag_response.cached
)
```

---

## ğŸ“Š Ø®Ù„Ø§ØµÙ‡ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ LLM

| Ù…Ø±Ø­Ù„Ù‡ | ÙØ§ÛŒÙ„ | Ù…ØªØ¯ | Prompt | Temperature | Max Tokens | Ù‡Ø¯Ù |
|-------|------|-----|--------|-------------|------------|-----|
| **1. Classification** | `llm/classifier.py` | `classify()` | Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø³ÙˆØ§Ù„ | 0.2 | 512 | ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø³ÙˆØ§Ù„ |
| **2. Query Enhancement** | `rag/pipeline.py` | `_enhance_query()` | Ø¨Ù‡Ø¨ÙˆØ¯ query | 0.1 | 200 | Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ |
| **3. Answer Generation** | `rag/pipeline.py` | `_generate_answer()` | ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ context | 0.4 | 4096 | Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ |

---

## ğŸ¯ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¬Ø±ÛŒØ§Ù† Ú©Ø§Ù…Ù„

```
1. API Request
   â†“
2. Authentication & User Check
   â†“
3. LLM Classification (business_question?)
   â†“ YES
4. Cache Check
   â†“ MISS
5. LLM Query Enhancement ("Ù…Ø§Ø¯Ù‡ Ø¯Ù‡" â†’ "Ù…Ø§Ø¯Ù‡ 10")
   â†“
6. Generate Embedding (1024 dims)
   â†“
7. Qdrant Vector Search (score_threshold=0.4)
   â†“
8. Retrieve 15 chunks
   â†“
9. Rerank to top 5
   â†“
10. Build Context from chunks
   â†“
11. LLM Answer Generation (with context)
   â†“
12. Extract Sources
   â†“
13. Save to Database
   â†“
14. Send to Users System (Celery)
   â†“
15. Return Response
```

---

## âš¡ Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ

1. **3 Ø¨Ø§Ø± LLM ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯**: Classification â†’ Enhancement â†’ Generation
2. **Qdrant ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø±**: Vector search Ø¨Ø§ threshold=0.4
3. **Reranking ÙØ¹Ù„Ø§Ù‹ ØºÛŒØ±ÙØ¹Ø§Ù„**: Ú†ÙˆÙ† Cohere API key Ø®Ø§Ù„ÛŒ Ø§Ø³Øª
4. **Cache Ø¯Ø± Redis**: Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨Ø®Ø´ÛŒØ¯Ù† Ø¨Ù‡ queries ØªÚ©Ø±Ø§Ø±ÛŒ
5. **Background tasks**: Ø§Ø±Ø³Ø§Ù„ Ù†ØªÛŒØ¬Ù‡ Ø¨Ù‡ Users system Ø¨Ø¯ÙˆÙ† ØªØ£Ø®ÛŒØ± Ø¯Ø± response

---

## ğŸ”§ ØªØºÛŒÛŒØ±Ø§Øª Ø§Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ù…Ø´Ú©Ù„ RAG

### 1. Ø±ÙØ¹ Ù…Ø´Ú©Ù„ Hybrid Search
- Ø­Ø°Ù keyword search Ù†Ø§Ø¯Ø±Ø³Øª Ú©Ù‡ ÙÙ‚Ø· exact match Ø±Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² vector search Ø®Ø§Ù„Øµ Ø¨Ø§ threshold=0.4

### 2. Ú©Ø§Ù‡Ø´ Similarity Threshold
- ØªØºÛŒÛŒØ± Ø§Ø² 0.7 Ø¨Ù‡ 0.5 Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ recall

### 3. Ø¨Ù‡Ø¨ÙˆØ¯ Query Enhancement
- ØªØºÛŒÛŒØ± Ø§Ø² hardcoded replacements Ø¨Ù‡ LLM-based enhancement
- Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² ØªÙ…Ø§Ù… Ø§Ø®ØªØµØ§Ø±Ø§ØªØŒ ØªØµØ­ÛŒØ­ Ø§Ù…Ù„Ø§ØŒ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯

### 4. Ø§ÙØ²ÙˆØ¯Ù† Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Debug
- Ù„Ø§Ú¯ ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ø§Ù…ØªÛŒØ§Ø² chunks Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡
- Ù„Ø§Ú¯ Ù†Ø§Ù… Ø§Ø³Ù†Ø§Ø¯ Ùˆ Ù…Ù†Ø§Ø¨Ø¹
- Ù„Ø§Ú¯ query enhancement

---

## ğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ

| ÙØ§ÛŒÙ„ | Ù†Ù‚Ø´ |
|------|-----|
| `/srv/app/api/v1/endpoints/query.py` | Ù†Ù‚Ø·Ù‡ ÙˆØ±ÙˆØ¯ API |
| `/srv/app/rag/pipeline.py` | Ù‡Ø³ØªÙ‡ Ø§ØµÙ„ÛŒ RAG pipeline |
| `/srv/app/llm/classifier.py` | Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø³ÙˆØ§Ù„Ø§Øª |
| `/srv/app/llm/openai_provider.py` | Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ OpenAI API |
| `/srv/app/services/qdrant_service.py` | Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± vector database |
| `/srv/app/services/embedding_service.py` | ØªÙˆÙ„ÛŒØ¯ embeddings |
| `/srv/.env` | ØªÙ†Ø¸ÛŒÙ…Ø§Øª (thresholds, API keys) |
