"""
RAG Pipeline
Complete Retrieval-Augmented Generation pipeline
"""

from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import asyncio
import hashlib
import json
import re

import structlog
import pytz
import jdatetime
from tenacity import retry, stop_after_attempt, wait_exponential

from app.services.qdrant_service import QdrantService
from app.services.embedding_service import get_embedding_service
from app.services.reranker_service import get_reranker
from app.llm.base import Message
from app.llm.classifier import QueryClassifier
from app.llm.factory import create_llm2_pro
from app.core.dependencies import get_redis_client
from app.config.settings import settings
from app.config.prompts import (
    RAGPrompts,
    SystemPrompts,
    QueryEnhancementPrompts,
)

logger = structlog.get_logger()


@dataclass
class RAGQuery:
    """RAG query request."""
    text: str
    user_id: str
    conversation_id: Optional[str] = None
    language: str = "fa"
    max_chunks: int = None  # Ø§Ú¯Ø± None Ø¨Ø§Ø´Ø¯ Ø§Ø² settings.rag_max_chunks Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
    filters: Optional[Dict[str, Any]] = None
    use_cache: bool = True
    use_reranking: bool = True
    user_preferences: Optional[Dict[str, Any]] = None
    enable_web_search: bool = False
    # ÙÛŒÙ„ØªØ± Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ù‚ÙˆØ§Ù†ÛŒÙ†
    temporal_context: Optional[str] = None  # "current" ÛŒØ§ "past" ÛŒØ§ None
    target_date: Optional[str] = None  # ØªØ§Ø±ÛŒØ® Ù‡Ø¯Ù Ø¨Ø±Ø§ÛŒ Ú¯Ø°Ø´ØªÙ‡ (YYYY-MM-DD)
    
    def __post_init__(self):
        """Set default values from settings if not provided."""
        if self.max_chunks is None:
            self.max_chunks = settings.rag_max_chunks


@dataclass
class RAGChunk:
    """Retrieved document chunk."""
    text: str
    score: float
    source: str
    metadata: Dict[str, Any]
    document_id: Optional[str] = None


@dataclass
class RAGResponse:
    """RAG pipeline response."""
    answer: str
    chunks: List[RAGChunk]
    sources: List[str]
    total_tokens: int
    processing_time_ms: int
    cached: bool = False
    model_used: str = ""
    input_tokens: int = 0
    output_tokens: int = 0
    reranker_details: Optional[List[Dict[str, Any]]] = None  # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ reranker


class RAGPipeline:
    """Complete RAG pipeline for question answering."""
    
    def __init__(self):
        """Initialize RAG pipeline components."""
        self.qdrant = QdrantService()
        # Use unified embedding service (auto-detects API vs local)
        self.embedder = get_embedding_service()
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LLM2 (Pro) Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±
        self.llm = create_llm2_pro()
        self.classifier = QueryClassifier()  # LLM Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø³ÙˆØ§Ù„Ø§Øª
        self.reranker = get_reranker()  # Initialize Cohere reranker if configured
        if self.reranker:
            logger.info("RAG Pipeline initialized with LLM2 (Pro) and Cohere Reranker")
        else:
            logger.info("RAG Pipeline initialized with LLM2 (Pro) (no reranker)")
        
    async def process(
        self, 
        query: RAGQuery, 
        additional_context: str = None, 
        skip_classification: bool = False,
        image_urls: List[str] = None
    ) -> RAGResponse:
        """
        Process a query through the RAG pipeline.
        
        Args:
            query: RAG query request
            additional_context: Additional context for LLM (memory, file analysis, etc.)
            skip_classification: Skip classification if already done in query endpoint
            image_urls: List of presigned URLs for images to send to LLM
            
        Returns:
            RAG response with answer and sources
        """
        start_time = datetime.utcnow()
        
        try:
            # Step 0: Classify query using LLM (if enabled and not skipped)
            # ØªÙˆØ¬Ù‡: Ø§Ú¯Ø± Ø§Ø² query.py Ø¢Ù…Ø¯Ù‡ØŒ classification Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ Ùˆ skip Ù…ÛŒâ€ŒØ´ÙˆØ¯
            classification = None
            if settings.enable_query_classification and not skip_classification:
                classification = await self.classifier.classify(query.text, query.language)
                
                logger.info(
                    "Query classified",
                    category=classification.category,
                    confidence=classification.confidence
                )
            
            # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù:
            # 1. invalid_no_file, invalid_with_file â†’ Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² Ú©Ù„Ø§Ø³ÛŒÙÛŒÚ©ÛŒØ´Ù† (direct_response)
            # 2. general â†’ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ LLM1 Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø¹Ù…ÙˆÙ…ÛŒ (Ø¨Ø¯ÙˆÙ† RAG)
            # 3. business_no_file, business_with_file â†’ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ù‡ RAG
            
            invalid_categories = ["invalid_no_file", "invalid_with_file"]
            if classification and classification.category in invalid_categories:
                # Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ù†Ø§Ù…Ø¹ØªØ¨Ø±
                processing_time = int((datetime.utcnow() - start_time).total_seconds() * 1000)
                return RAGResponse(
                    answer=classification.direct_response or "Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø¶Ø­â€ŒØªØ± Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯.",
                    chunks=[],
                    sources=[],
                    total_tokens=0,
                    processing_time_ms=processing_time,
                    cached=False,
                    model_used="classifier"
                )
            
            if classification and classification.category == "general":
                # Ø³ÙˆØ§Ù„Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ â†’ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ LLM1 Ø¨Ø¯ÙˆÙ† RAG
                processing_time_start = datetime.utcnow()
                try:
                    llm_response = await self._generate_general_response(query.text)
                    processing_time = int((datetime.utcnow() - start_time).total_seconds() * 1000)
                    return RAGResponse(
                        answer=llm_response,
                        chunks=[],
                        sources=[],
                        total_tokens=0,
                        processing_time_ms=processing_time,
                        cached=False,
                        model_used="llm1_general"
                    )
                except Exception as e:
                    logger.error(f"Error generating general response: {e}")
                    # Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ØŒ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ù‡ RAG
                    pass
            
            # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
            
            # Check cache if enabled
            if query.use_cache:
                cached_response = await self._check_cache(query)
                if cached_response:
                    cached_response.cached = True
                    return cached_response
            
            # Step 1: Query understanding and enhancement
            enhanced_query = await self._enhance_query(query)
            
            # Step 2: Generate embedding
            query_embedding = await self._generate_embedding(enhanced_query)
            
            # Step 3: Retrieve relevant chunks
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¶Ø±ÛŒØ¨ ØªÙ†Ø¸ÛŒÙ…â€ŒØ´Ø¯Ù‡ Ø¯Ø± settings Ø¨Ø±Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ chunks Ø§ÙˆÙ„ÛŒÙ‡
            retrieve_limit = query.max_chunks * settings.rag_retrieve_multiplier
            chunks = await self._retrieve_chunks(
                query_embedding,
                enhanced_query,
                query.filters,
                limit=retrieve_limit
            )
            
            logger.info(
                "Retrieved chunks",
                query=query.text[:100],
                enhanced_query=enhanced_query[:100],
                num_chunks=len(chunks),
                top_scores=[c.score for c in chunks[:3]] if chunks else []
            )
            
            # Step 3.5: ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ® Ø§Ø¹ØªØ¨Ø§Ø± Ù‚ÙˆØ§Ù†ÛŒÙ†
            if query.temporal_context:
                chunks = self._filter_chunks_by_validity(
                    chunks,
                    query.temporal_context,
                    query.target_date
                )
            
            # Step 4: Rerank if enabled
            reranker_details = []
            if query.use_reranking and len(chunks) > query.max_chunks:
                chunks, reranker_details = await self._rerank_chunks(
                    enhanced_query,
                    chunks,
                    top_k=query.max_chunks
                )
                logger.info(
                    "Reranked chunks",
                    final_count=len(chunks),
                    top_scores=[c.score for c in chunks[:3]] if chunks else []
                )
            else:
                chunks = chunks[:query.max_chunks]
                logger.info(
                    "Using top chunks without reranking",
                    count=len(chunks)
                )
            
            # Step 4.5: Expand legal context for lunit nodes
            chunks = await self._expand_legal_context(chunks)
            logger.info(
                "Context expansion completed",
                final_count=len(chunks)
            )
            
            # Step 5: Generate answer
            logger.info(
                "Generating answer",
                num_chunks=len(chunks),
                chunk_sources=[c.metadata.get('work_title', 'N/A')[:50] for c in chunks[:3]]
            )
            
            answer, tokens_used, input_tokens, output_tokens = await self._generate_answer(
                query.text,
                chunks,
                query.language,
                query.conversation_id,
                query.user_preferences,
                additional_context=additional_context,
                enable_web_search=query.enable_web_search,
                image_urls=image_urls
            )
            
            # Step 6: Extract sources (filter based on LLM's decision)
            # Ø§Ú¯Ø± LLM ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯ Ú©Ù‡ Ù‚Ø§Ù†ÙˆÙ†/Ù…Ø§Ø¯Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ù…Ù†Ø§Ø¨Ø¹ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù†Ø´ÙˆÙ†Ø¯
            if answer.startswith("[NO_SOURCES]"):
                # Ø­Ø°Ù ØªÚ¯ Ø§Ø² Ù¾Ø§Ø³Ø® Ùˆ Ø®Ø§Ù„ÛŒ Ú©Ø±Ø¯Ù† Ù…Ù†Ø§Ø¨Ø¹
                answer = answer.replace("[NO_SOURCES]", "").strip()
                sources = []
                chunks = []
                logger.info("LLM indicated no sources should be shown (non-existent law/article)")
            else:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· LLM
                answer, used_source_indices = self._extract_used_sources(answer)
                
                if used_source_indices is not None:
                    if len(used_source_indices) == 0:
                        # LLM Ú¯ÙØªÙ‡ Ù‡ÛŒÚ† Ù…Ù†Ø¨Ø¹ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡
                        chunks = []
                        sources = []
                        logger.info("LLM indicated no sources were used")
                    else:
                        # ÙÛŒÙ„ØªØ± chunks Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡
                        filtered_chunks = []
                        for idx in used_source_indices:
                            if 0 < idx <= len(chunks):
                                filtered_chunks.append(chunks[idx - 1])  # ØªØ¨Ø¯ÛŒÙ„ 1-indexed Ø¨Ù‡ 0-indexed
                        
                        logger.info(
                            "Filtered sources based on LLM decision",
                            original_count=len(chunks),
                            used_indices=used_source_indices,
                            filtered_count=len(filtered_chunks)
                        )
                        chunks = filtered_chunks
                        sources = self._extract_sources(chunks)
                else:
                    # Ø§Ú¯Ø± LLM ØªÚ¯ Ø±Ø§ Ù†Ù†ÙˆØ´ØªØŒ Ù‡Ù…Ù‡ Ù…Ù†Ø§Ø¨Ø¹ Ø±Ø§ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ… (backward compatibility)
                    sources = self._extract_sources(chunks)
                    logger.warning("LLM did not specify used sources, keeping all")
            
            # Calculate processing time
            processing_time = int(
                (datetime.utcnow() - start_time).total_seconds() * 1000
            )
            
            # Create response
            response = RAGResponse(
                answer=answer,
                chunks=chunks,
                sources=sources,
                total_tokens=tokens_used,
                processing_time_ms=processing_time,
                model_used=self.llm.config.model,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                reranker_details=reranker_details
            )
            
            # Cache response if enabled
            if query.use_cache:
                await self._cache_response(query, response)
            
            return response
            
        except Exception as e:
            logger.error(f"RAG pipeline error: {e}")
            raise
    
    async def _generate_general_response(self, query_text: str) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ (ØºÛŒØ± ØªØ®ØµØµÛŒ) Ø¨Ø¯ÙˆÙ† RAG."""
        system_prompt = SystemPrompts.get_general_question_prompt()
        messages = [
            Message(role="system", content=system_prompt),
            Message(role="user", content=query_text)
        ]
        
        response = await self.llm.generate_responses_api(
            messages=messages, reasoning_effort="low", max_tokens=500
        )
        return response.content
    
    async def _enhance_query(self, query: RAGQuery) -> str:
        """
        Enhance query for better retrieval using LLM.
        
        Args:
            query: Original query
            
        Returns:
            Enhanced query text
        """
        if query.language != "fa":
            return query.text
        
        try:
            system_prompt = QueryEnhancementPrompts.get_enhancement_prompt(query.language)
            messages = [
                Message(role="system", content=system_prompt),
                Message(role="user", content=f"Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {query.text}")
            ]
            
            response = await self.llm.generate_responses_api(
                messages, reasoning_effort="low", max_tokens=200
            )
            
            enhanced = response.content.strip()
            
            # Ø§Ú¯Ø± LLM Ú†ÛŒØ² Ø¹Ø¬ÛŒØ¨ÛŒ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯ØŒ Ø§Ø² query Ø§ØµÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
            if not enhanced or len(enhanced) > len(query.text) * 3:
                logger.warning("LLM enhancement failed, using original query")
                return query.text
            
            logger.info(f"Query enhanced: '{query.text}' -> '{enhanced}'")
            return enhanced
            
        except Exception as e:
            logger.warning(f"Query enhancement failed: {e}")
            return query.text
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    async def _generate_embedding(self, text: str) -> List[float]:
        """
        Generate embedding for text with retry logic.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector
        """
        # Local embedding is synchronous, wrap it in async
        loop = asyncio.get_event_loop()
        embedding = await loop.run_in_executor(
            None, self.embedder.encode_single, text
        )
        return embedding.tolist()
    
    async def _retrieve_chunks(
        self,
        query_embedding: List[float],
        query_text: str,
        filters: Optional[Dict[str, Any]],
        limit: int
    ) -> List[RAGChunk]:
        """
        Retrieve relevant chunks from vector database.
        
        Args:
            query_embedding: Query embedding vector
            query_text: Query text for hybrid search
            filters: Optional filters
            limit: Maximum chunks to retrieve
            
        Returns:
            List of relevant chunks
        """
        # Determine vector field based on embedding dimension
        vector_field = self._get_vector_field(len(query_embedding))
        
        # Perform hybrid search if enabled
        if settings.rag_use_hybrid_search:
            results = await self.qdrant.hybrid_search(
                query_vector=query_embedding,
                query_text=query_text,
                limit=limit,
                vector_weight=settings.rag_vector_weight,
                keyword_weight=settings.rag_bm25_weight,
                filters=filters,
                vector_field=vector_field
            )
        else:
            # Vector-only search
            results = await self.qdrant.search(
                query_vector=query_embedding,
                limit=limit,
                score_threshold=settings.rag_similarity_threshold,
                filters=filters,
                vector_field=vector_field
            )
        
        # Convert to RAGChunk objects
        chunks = []
        for result in results:
            chunk = RAGChunk(
                text=result["text"],
                score=result.get("score", 0.0),
                source=result.get("source", "unknown"),
                metadata=result.get("metadata", {}),
                document_id=result.get("document_id")
            )
            chunks.append(chunk)
        
        logger.debug(
            "Retrieved chunks from Qdrant",
            count=len(chunks),
            vector_field=vector_field,
            top_3_docs=[c.metadata.get('work_title', 'N/A')[:30] for c in chunks[:3]]
        )
        
        return chunks
    
    def _filter_chunks_by_validity(
        self,
        chunks: List[RAGChunk],
        temporal_context: Optional[str],
        target_date: Optional[str]
    ) -> List[RAGChunk]:
        """
        ÙÛŒÙ„ØªØ± chunks Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ® Ø§Ø¹ØªØ¨Ø§Ø± Ù‚ÙˆØ§Ù†ÛŒÙ†.
        
        Args:
            chunks: Ù„ÛŒØ³Øª chunks
            temporal_context: "current" (Ù‚ÙˆØ§Ù†ÛŒÙ† Ù…Ø¹ØªØ¨Ø± Ø§Ù…Ø±ÙˆØ²) ÛŒØ§ "past" (Ù‚ÙˆØ§Ù†ÛŒÙ† Ù…Ø¹ØªØ¨Ø± Ø¯Ø± ØªØ§Ø±ÛŒØ® Ù‡Ø¯Ù)
            target_date: ØªØ§Ø±ÛŒØ® Ù‡Ø¯Ù Ø¨Ø±Ø§ÛŒ Ú¯Ø°Ø´ØªÙ‡ (YYYY-MM-DD)
            
        Returns:
            chunks ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡
        """
        if not temporal_context:
            return chunks
        
        # ØªØ¹ÛŒÛŒÙ† ØªØ§Ø±ÛŒØ® Ù…Ø±Ø¬Ø¹
        if temporal_context == "current":
            reference_date = datetime.utcnow().date()
        elif temporal_context == "past" and target_date:
            try:
                reference_date = datetime.strptime(target_date, "%Y-%m-%d").date()
            except ValueError:
                logger.warning(f"Invalid target_date format: {target_date}, using current date")
                reference_date = datetime.utcnow().date()
        else:
            return chunks
        
        filtered_chunks = []
        excluded_count = 0
        
        for chunk in chunks:
            metadata = chunk.metadata
            
            # Ø¨Ø±Ø±Ø³ÛŒ is_active (Ø§ÙˆÙ„ÙˆÛŒØª Ø§ÙˆÙ„)
            is_active = metadata.get("is_active")
            
            # Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª "current": Ø§Ú¯Ø± is_active=FalseØŒ Ø­Ø°Ù Ú©Ù†
            if temporal_context == "current" and is_active is False:
                excluded_count += 1
                logger.debug(
                    "Excluded chunk: is_active=False",
                    work_title=metadata.get("work_title", "")[:30]
                )
                continue
            
            # Ø¨Ø±Ø±Ø³ÛŒ valid_from Ùˆ valid_to
            valid_from_str = metadata.get("valid_from")
            valid_to_str = metadata.get("valid_to")
            
            # Ø§Ú¯Ø± ÙÛŒÙ„Ø¯ ØªØ§Ø±ÛŒØ® Ø§Ø¹ØªØ¨Ø§Ø± Ù†Ø¯Ø§Ø´ØªØŒ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ…
            if not valid_from_str:
                filtered_chunks.append(chunk)
                continue
            
            try:
                # Ù¾Ø§Ø±Ø³ ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§
                valid_from = datetime.strptime(valid_from_str[:10], "%Y-%m-%d").date()
                valid_to = None
                if valid_to_str:
                    valid_to = datetime.strptime(valid_to_str[:10], "%Y-%m-%d").date()
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¹ØªØ¨Ø§Ø± Ø¯Ø± ØªØ§Ø±ÛŒØ® Ù…Ø±Ø¬Ø¹
                # Ù‚Ø§Ù†ÙˆÙ† Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª Ø§Ú¯Ø±: valid_from <= reference_date AND (valid_to is None OR valid_to > reference_date)
                is_valid = valid_from <= reference_date
                if valid_to:
                    is_valid = is_valid and valid_to > reference_date
                
                if is_valid:
                    filtered_chunks.append(chunk)
                else:
                    excluded_count += 1
                    logger.debug(
                        "Excluded chunk due to validity",
                        work_title=metadata.get("work_title", "")[:30],
                        valid_from=valid_from_str,
                        valid_to=valid_to_str,
                        reference_date=str(reference_date)
                    )
            except (ValueError, TypeError) as e:
                # Ø§Ú¯Ø± Ù¾Ø§Ø±Ø³ ØªØ§Ø±ÛŒØ® Ø®Ø·Ø§ Ø¯Ø§Ø¯ØŒ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ…
                filtered_chunks.append(chunk)
        
        if excluded_count > 0:
            logger.info(
                "Filtered chunks by validity date",
                temporal_context=temporal_context,
                reference_date=str(reference_date),
                original_count=len(chunks),
                filtered_count=len(filtered_chunks),
                excluded_count=excluded_count
            )
        
        return filtered_chunks
    
    async def _rerank_chunks(
        self,
        query: str,
        chunks: List[RAGChunk],
        top_k: int
    ) -> Tuple[List[RAGChunk], List[Dict[str, Any]]]:
        """
        Rerank chunks for better relevance.
        
        Args:
            query: Query text
            chunks: Retrieved chunks
            top_k: Number of top chunks to return
            
        Returns:
            Tuple of (reranked_chunks, reranker_details)
            reranker_details contains full info about all chunks before filtering
        """
        reranker_details = []
        
        if not chunks:
            return [], []
        
        # If we have reranker configured (local Docker service or Cohere API)
        if self.reranker:
            try:
                # Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù‡Ù…Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¯Ù† Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ú©Ø§Ù…Ù„
                reranked = await self.reranker.rerank(
                    query=query,
                    documents=[c.text for c in chunks],
                    top_k=len(chunks)  # Ù‡Ù…Ù‡ Ø±Ø§ Ø¨Ú¯ÛŒØ±
                )
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ Ù‡Ù…Ù‡ chunks
                for idx, score in reranked:
                    chunk = chunks[idx]
                    reranker_details.append({
                        "original_index": idx,
                        "score": round(score, 4),
                        "source": chunk.metadata.get("work_title", "")[:50],
                        "unit": chunk.metadata.get("unit_number", ""),
                        "text_preview": chunk.text[:100] + "..." if len(chunk.text) > 100 else chunk.text
                    })
                
                # Reorder chunks based on reranking - ÙÙ‚Ø· top_k Ø¨Ø§ Ø§Ø¹Ù…Ø§Ù„ threshold
                threshold = settings.rag_reranker_threshold
                reranked_chunks = []
                for idx, score in reranked[:top_k]:
                    # Ø§Ú¯Ø± threshold ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡ØŒ chunks Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù…ØªØ± Ø±Ø§ Ø­Ø°Ù Ú©Ù†
                    if threshold > 0 and score < threshold:
                        continue
                    chunk = chunks[idx]
                    chunk.score = score  # Update score with rerank score
                    reranked_chunks.append(chunk)
                
                logger.info(
                    "Reranking completed",
                    original_count=len(chunks),
                    reranked_count=len(reranked_chunks),
                    threshold=threshold,
                    filtered_by_threshold=top_k - len(reranked_chunks),
                    all_scores=[d["score"] for d in reranker_details],
                    top_scores=[c.score for c in reranked_chunks[:3]]
                )
                
                return reranked_chunks, reranker_details
                
            except Exception as e:
                logger.warning(f"Reranking failed, using original order: {e}")
        
        # Fallback: Simple score-based reranking
        sorted_chunks = sorted(chunks, key=lambda x: x.score, reverse=True)[:top_k]
        return sorted_chunks, []
    
    async def _expand_legal_context(self, chunks: List[RAGChunk]) -> List[RAGChunk]:
        """
        ØªÙˆØ³Ø¹Ù‡ context Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ¯Ù‡Ø§ÛŒ lunit (Ù…ÙˆØ§Ø¯ Ø­Ù‚ÙˆÙ‚ÛŒ).
        Ø§Ú¯Ø± chunk ÛŒÚ© Ø¨Ø®Ø´ Ø§Ø² ÛŒÚ© Ù…Ø§Ø¯Ù‡ Ø­Ù‚ÙˆÙ‚ÛŒ Ø§Ø³ØªØŒ ØªÙ…Ø§Ù… Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¢Ù† Ù…Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        
        ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ document_type="lunit" Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        Ø¨Ø±Ø§ÛŒ qaentry Ùˆ textentry Ù‡ÛŒÚ† Ú©Ø§Ø±ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        
        Args:
            chunks: Ù„ÛŒØ³Øª chunks Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡
            
        Returns:
            chunks Ø¨Ø§ context ØªÙˆØ³Ø¹Ù‡ ÛŒØ§ÙØªÙ‡
        """
        if not chunks:
            return chunks
        
        expanded_chunks = []
        seen_articles = set()  # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø±: (document_id, unit_number)
        expansion_count = 0
        
        for chunk in chunks:
            metadata = chunk.metadata
            
            # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ lunit nodes
            doc_type = metadata.get("document_type", "")
            if doc_type != "lunit":
                # qaentry Ùˆ textentry Ø±Ø§ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
                expanded_chunks.append(chunk)
                continue
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø§Ø¯Ù‡
            document_id = metadata.get("document_id")
            unit_number = metadata.get("unit_number")
            
            if not document_id or not unit_number:
                # Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ Ù†ÛŒØ³ØªØŒ Ù‡Ù…Ø§Ù† chunk Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
                expanded_chunks.append(chunk)
                continue
            
            # Ú©Ù„ÛŒØ¯ ÛŒÚ©ØªØ§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ø§Ø¯Ù‡
            article_key = f"{document_id}_{unit_number}"
            
            if article_key in seen_articles:
                # Ø§ÛŒÙ† Ù…Ø§Ø¯Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ØŒ Ø±Ø¯ Ø´Ùˆ
                continue
            
            seen_articles.add(article_key)
            
            # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ ØªÙ…Ø§Ù… chunks Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§ÛŒÙ† Ù…Ø§Ø¯Ù‡ Ø§Ø² Qdrant
            try:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² scroll Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ ØªÙ…Ø§Ù… chunks Ø¨Ø§ ÙÛŒÙ„ØªØ±
                from qdrant_client.models import Filter, FieldCondition, MatchValue
                
                scroll_filter = Filter(
                    must=[
                        FieldCondition(
                            key="document_id",
                            match=MatchValue(value=document_id)
                        ),
                        FieldCondition(
                            key="unit_number",
                            match=MatchValue(value=unit_number)
                        )
                    ]
                )
                
                # Scroll through all matching points
                scroll_result = self.qdrant.client.scroll(
                    collection_name=self.qdrant.collection_name,
                    scroll_filter=scroll_filter,
                    limit=100,  # Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ chunks Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù…Ø§Ø¯Ù‡
                    with_payload=True,
                    with_vectors=False
                )
                
                article_points = scroll_result[0]  # Ù„ÛŒØ³Øª points
                
                if not article_points:
                    # Ø§Ú¯Ø± Ú†ÛŒØ²ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ù‡Ù…Ø§Ù† chunk Ø§ØµÙ„ÛŒ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
                    expanded_chunks.append(chunk)
                    continue
                
                # ØªØ¨Ø¯ÛŒÙ„ points Ø¨Ù‡ RAGChunk Ùˆ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ chunk_index
                article_chunks = []
                for point in article_points:
                    payload = point.payload
                    article_chunks.append(RAGChunk(
                        text=payload.get("text", ""),
                        score=chunk.score,  # Ù‡Ù…Ø§Ù† score chunk Ø§ØµÙ„ÛŒ
                        source=payload.get("source", ""),
                        metadata=payload,
                        document_id=payload.get("document_id")
                    ))
                
                # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ chunk_index
                article_chunks.sort(key=lambda x: x.metadata.get("chunk_index", 0))
                
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù„ÛŒØ³Øª Ù†Ù‡Ø§ÛŒÛŒ
                expanded_chunks.extend(article_chunks)
                expansion_count += len(article_chunks) - 1  # ØªØ¹Ø¯Ø§Ø¯ chunks Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
                
                logger.debug(
                    "Expanded legal article",
                    document_id=document_id,
                    unit_number=unit_number,
                    work_title=metadata.get("work_title", "")[:50],
                    chunks_added=len(article_chunks)
                )
                
            except Exception as e:
                logger.warning(
                    f"Failed to expand legal context for article {unit_number}: {e}",
                    document_id=document_id,
                    unit_number=unit_number
                )
                # Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ØŒ Ù‡Ù…Ø§Ù† chunk Ø§ØµÙ„ÛŒ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
                expanded_chunks.append(chunk)
        
        if expansion_count > 0:
            logger.info(
                "Legal context expansion completed",
                original_chunks=len(chunks),
                expanded_chunks=len(expanded_chunks),
                additional_chunks=expansion_count,
                articles_expanded=len(seen_articles)
            )
        
        return expanded_chunks
    
    async def _generate_answer(
        self,
        query: str,
        chunks: List[RAGChunk],
        language: str,
        conversation_id: Optional[str],
        user_preferences: Optional[Dict[str, Any]] = None,
        additional_context: str = None,
        enable_web_search: bool = False,
        image_urls: List[str] = None
    ) -> Tuple[str, int, int, int]:
        """
        Generate answer using LLM with retrieved context.
        
        Args:
            query: User query
            chunks: Retrieved chunks
            language: Response language
            conversation_id: Optional conversation ID for context
            user_preferences: Optional user preferences for response customization
            additional_context: Additional context (memory, file analysis, etc.)
            enable_web_search: Enable web search to supplement RAG sources
            image_urls: List of presigned URLs for images to send to LLM
            
        Returns:
            Tuple of (answer, total_tokens, input_tokens, output_tokens)
        """
        # Build context from chunks
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            source_info = f"[Ù…Ù†Ø¨Ø¹ {i}]"
            work_title = chunk.metadata.get("work_title") or chunk.metadata.get("document_title")
            if work_title:
                source_info += f" {work_title}"
            if chunk.metadata.get("unit_number"):
                source_info += f" - Ù…Ø§Ø¯Ù‡ {chunk.metadata['unit_number']}"
            
            context_parts.append(f"{source_info}:\n{chunk.text}")
        
        context = "\n\n".join(context_parts)
        
        # Build system prompt
        system_prompt = self._build_system_prompt(language, user_preferences)
        
        # Build user message
        if language == "fa":
            user_message_parts = []
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† additional context (Ø­Ø§ÙØ¸Ù‡ØŒ ÙØ§ÛŒÙ„ØŒ Ùˆ...)
            if additional_context:
                user_message_parts.append(additional_context)
                user_message_parts.append("\n" + "="*50 + "\n")
            else:
                # Ø§Ú¯Ø± additional_context Ù†ÛŒØ³ØªØŒ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
                user_message_parts.append(f"[Ø³ÙˆØ§Ù„ ÙØ¹Ù„ÛŒ]\n{query}\n")
            
            user_message_parts.append(f"""Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±Ø¬Ø¹ Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡:
{context}""")
            
            user_message = "\n".join(user_message_parts)
        else:
            user_message_parts = []
            
            if additional_context:
                user_message_parts.append(additional_context)
                user_message_parts.append("\n" + "="*50 + "\n")
            else:
                # If no additional_context, add user query directly
                user_message_parts.append(f"[Current Question]\n{query}\n")
            
            user_message_parts.append(f"""Reference information from database:
{context}""")
            
            user_message = "\n".join(user_message_parts)
        
        # Add user preferences to the message if provided
        if user_preferences:
            prefs_text = self._format_user_preferences(user_preferences, language)
            if prefs_text:
                user_message += f"\n\n{prefs_text}"
        
        # Build messages
        messages = [
            Message(role="system", content=system_prompt),
            Message(role="user", content=user_message)
        ]
        
        # Generate response - Ø¨Ø§ ÛŒØ§ Ø¨Ø¯ÙˆÙ† web search Ùˆ ØªØµØ§ÙˆÛŒØ±
        if image_urls:
            # Ø§Ú¯Ø± ØªØµÙˆÛŒØ± Ø¯Ø§Ø±ÛŒÙ…ØŒ Ø§Ø² input_content Ø¨Ø§ input_image Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
            content_parts = [
                {"type": "input_text", "text": f"{system_prompt}\n\n---\n\n{user_message}"}
            ]
            for img_url in image_urls:
                content_parts.append({
                    "type": "input_image",
                    "image_url": img_url
                })
            
            input_content = [{"role": "user", "content": content_parts}]
            
            logger.info(f"Generating RAG answer with {len(image_urls)} images")
            response = await self.llm.generate_responses_api(
                messages=[],
                reasoning_effort="medium",
                input_content=input_content
            )
        elif enable_web_search:
            logger.info("Generating RAG answer with web search enabled")
            response = await self.llm.generate_with_web_search(messages)
        else:
            response = await self.llm.generate_responses_api(
                messages,
                reasoning_effort="medium"
            )
        
        # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
        input_tokens = response.usage.get("prompt_tokens", 0)
        output_tokens = response.usage.get("completion_tokens", 0)
        total_tokens = response.usage.get("total_tokens", input_tokens + output_tokens)
        
        return response.content, total_tokens, input_tokens, output_tokens
    
    def _build_system_prompt(self, language: str, user_preferences: Optional[Dict[str, Any]] = None) -> str:
        """Build system prompt based on language and user preferences."""
        # Get current date and time in Tehran timezone
        tehran_tz = pytz.timezone('Asia/Tehran')
        now = datetime.now(tehran_tz)
        jalali_now = jdatetime.datetime.fromgregorian(datetime=now)
        
        current_date_shamsi = jalali_now.strftime('%Y/%m/%d')
        current_time = now.strftime('%H:%M')
        
        if language == "fa":
            base_prompt = RAGPrompts.get_rag_system_prompt_fa(
                current_date_shamsi=current_date_shamsi,
                current_time_fa=current_time
            )
        else:
            base_prompt = RAGPrompts.get_rag_system_prompt_en(
                current_date_gregorian=now.strftime('%Y-%m-%d'),
                current_date_shamsi=current_date_shamsi,
                current_time=current_time
            )
        
        # Add user preferences if provided
        if user_preferences:
            base_prompt += self._format_preferences_for_prompt(user_preferences, language)
        
        return base_prompt
    
    def _format_preferences_for_prompt(self, prefs: Dict[str, Any], language: str) -> str:
        """Format user preferences for system prompt."""
        additions = []
        
        if prefs.get("response_style"):
            key = "Ø³Ø¨Ú© Ù¾Ø§Ø³Ø®" if language == "fa" else "Response style"
            additions.append(f"- {key}: {prefs['response_style']}")
        
        if prefs.get("detail_level"):
            key = "Ø³Ø·Ø­ Ø¬Ø²Ø¦ÛŒØ§Øª" if language == "fa" else "Detail level"
            additions.append(f"- {key}: {prefs['detail_level']}")
        
        if not additions:
            return ""
        
        header = "\n\nØªØ±Ø¬ÛŒØ­Ø§Øª Ú©Ø§Ø±Ø¨Ø±:\n" if language == "fa" else "\n\nUser preferences:\n"
        return header + "\n".join(additions)
    
    def _format_user_preferences(self, preferences: Dict[str, Any], language: str) -> str:
        """Format user preferences into a readable instruction for LLM."""
        if not preferences:
            return ""
        
        # Translation maps for Persian
        STYLE_MAP_FA = {
            "formal": "Ø±Ø³Ù…ÛŒ Ùˆ ØªØ®ØµØµÛŒ", "casual": "ØºÛŒØ±Ø±Ø³Ù…ÛŒ Ùˆ Ø³Ø§Ø¯Ù‡",
            "academic": "Ø¢Ú©Ø§Ø¯Ù…ÛŒÚ© Ùˆ Ø¹Ù„Ù…ÛŒ", "simple": "Ø³Ø§Ø¯Ù‡ Ùˆ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù…"
        }
        LEVEL_MAP_FA = {
            "brief": "Ø®Ù„Ø§ØµÙ‡ Ùˆ Ù…Ø®ØªØµØ±", "moderate": "Ù…ØªÙˆØ³Ø·",
            "comprehensive": "Ø¬Ø§Ù…Ø¹ Ùˆ Ú©Ø§Ù…Ù„", "detailed": "Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ú©Ø§Ù…Ù„"
        }
        FORMAT_MAP_FA = {
            "bullet_points": "Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯",
            "numbered_list": "Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù„ÛŒØ³Øª Ø´Ù…Ø§Ø±Ù‡â€ŒØ¯Ø§Ø± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯",
            "paragraph": "Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø³Ø¬Ù… Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯"
        }
        
        instructions = []
        is_fa = language == "fa"
        
        if preferences.get("response_style"):
            val = preferences["response_style"]
            label = STYLE_MAP_FA.get(val, val) if is_fa else val
            instructions.append(f"{'Ø³Ø¨Ú© Ù¾Ø§Ø³Ø®' if is_fa else 'Response style'}: {label}")
        
        if preferences.get("detail_level"):
            val = preferences["detail_level"]
            label = LEVEL_MAP_FA.get(val, val) if is_fa else val
            instructions.append(f"{'Ø³Ø·Ø­ Ø¬Ø²Ø¦ÛŒØ§Øª' if is_fa else 'Detail level'}: {label}")
        
        if preferences.get("include_examples"):
            instructions.append("Ù„Ø·ÙØ§Ù‹ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯" if is_fa else "Please include practical examples")
        
        if preferences.get("format"):
            val = preferences["format"]
            label = FORMAT_MAP_FA.get(val, val) if is_fa else val
            instructions.append(label if is_fa else f"Format: {label}")
        
        if not instructions:
            return ""
        
        header = "Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù¾Ø§Ø³Ø®:\n" if is_fa else "Response guidelines:\n"
        return header + "\n".join(f"- {inst}" for inst in instructions)
    
    def _extract_sources(self, chunks: List[RAGChunk]) -> List[str]:
        """Extract detailed sources from chunks with full context."""
        sources = []
        seen = set()
        source_number = 0  # Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ ØµØ­ÛŒØ­ Ù…Ù†Ø§Ø¨Ø¹
        
        for chunk in chunks:
            metadata = chunk.metadata
            
            # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ document_id + unit_number
            source_key = f"{metadata.get('document_id', '')}_{metadata.get('unit_number', '')}"
            if source_key in seen:
                continue  # Ø§ÛŒÙ† chunk ØªÚ©Ø±Ø§Ø±ÛŒ Ø§Ø³ØªØŒ Ø±Ø¯ Ø´Ùˆ
            seen.add(source_key)
            
            # Ø§ÙØ²Ø§ÛŒØ´ Ø´Ù…Ø§Ø±Ù‡ Ù…Ù†Ø¨Ø¹ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù…Ù†Ø§Ø¨Ø¹ ØºÛŒØ±ØªÚ©Ø±Ø§Ø±ÛŒ
            source_number += 1
            source_lines = []
            
            # 1. Ø´Ù…Ø§Ø±Ù‡ Ù…Ù†Ø¨Ø¹ Ùˆ Ù…ØªÙ† Ú©Ø§Ù…Ù„
            source_lines.append(f"ğŸ“Œ Ù…Ù†Ø¨Ø¹ {source_number}:")
            source_lines.append(f"ğŸ“„ Ù…ØªÙ†: {chunk.text}")
            source_lines.append("")  # Ø®Ø· Ø®Ø§Ù„ÛŒ
            
            # 2. Ù†Ø§Ù… Ù‚Ø§Ù†ÙˆÙ†/Ø³Ù†Ø¯ Ùˆ Ù†ÙˆØ¹
            doc_type = metadata.get("document_type") or metadata.get("doc_type", "")
            doc_title = metadata.get("document_title", "")
            unit_type = metadata.get("unit_type", "")
            
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² work_title Ø¨Ù‡ Ø¬Ø§ÛŒ document_title
            work_title = metadata.get("work_title", "")
            if not work_title:
                work_title = doc_title
            
            if work_title:
                source_lines.append(f"ğŸ“š Ù†Ø§Ù… Ø³Ù†Ø¯: {work_title}")
                if doc_type and doc_type != work_title:
                    source_lines.append(f"ğŸ“‹ Ù†ÙˆØ¹: {doc_type}")
            
            # 3. Ù…Ø³ÛŒØ± Ø¯Ù‚ÛŒÙ‚ (Ø§Ø² path_label ÛŒØ§ Ø³Ø§Ø®Øª Ø¯Ø³ØªÛŒ)
            path_label = metadata.get("path_label", "")
            
            if path_label:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø³ÛŒØ± Ú©Ø§Ù…Ù„ Ø§Ø² metadata
                source_lines.append(f"ğŸ“ Ù…Ø³ÛŒØ±: {path_label}")
            else:
                # Ø³Ø§Ø®Øª Ù…Ø³ÛŒØ± Ø§Ø² ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
                unit_number = metadata.get("unit_number")
                title = metadata.get("title", "")
                
                if unit_number:
                    if unit_type == "article":
                        source_lines.append(f"ğŸ“ Ù…Ø§Ø¯Ù‡ {unit_number}")
                    elif unit_type:
                        source_lines.append(f"ğŸ“ {unit_type} {unit_number}")
                    else:
                        source_lines.append(f"ğŸ“ Ù…Ø§Ø¯Ù‡ {unit_number}")
                
                if title and title != work_title:
                    source_lines.append(f"   Ø¹Ù†ÙˆØ§Ù†: {title}")
            
            # 4. Ù…Ø±Ø¬Ø¹ ØªØµÙˆÛŒØ¨ (ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ ØºÛŒØ± Ù‚ÙˆØ§Ù†ÛŒÙ†)
            authority = metadata.get("authority", "")
            
            # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø³Ù†Ø¯ - Ø§Ú¯Ø± Ù‚Ø§Ù†ÙˆÙ† Ø§Ø³ØªØŒ Ù…Ø±Ø¬Ø¹ ØªØµÙˆÛŒØ¨ Ù†Ù…Ø§ÛŒØ´ Ù†Ø¯Ù‡
            is_law = work_title and ("Ù‚Ø§Ù†ÙˆÙ†" in work_title.lower())
            
            # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø¨Ø®Ø´Ù†Ø§Ù…Ù‡/Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡/Ø±Ø§ÛŒ Ù…Ø±Ø¬Ø¹ ØªØµÙˆÛŒØ¨ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
            if authority and not is_law:
                source_lines.append(f"âœ… Ù…Ø±Ø¬Ø¹ ØªØµÙˆÛŒØ¨: {authority}")
            
            # Ø³Ø§Ø®Øª source Ù†Ù‡Ø§ÛŒÛŒ
            source = "\n".join(source_lines)
            sources.append(source)
        
        return sources
    
    def _extract_used_sources(self, answer: str) -> Tuple[str, Optional[List[int]]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù…Ø§Ø±Ù‡ Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø² Ù¾Ø§Ø³Ø® LLM."""
        # Ø§Ù„Ú¯ÙˆÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ ØªÚ¯ USED_SOURCES
        pattern = r'\[USED_SOURCES:\s*([^\]]+)\]'
        match = re.search(pattern, answer, re.IGNORECASE)
        
        if not match:
            return answer, None
        
        # Ø­Ø°Ù ØªÚ¯ Ø§Ø² Ù¾Ø§Ø³Ø®
        cleaned_answer = re.sub(pattern, '', answer, flags=re.IGNORECASE).strip()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ ØªÚ¯
        content = match.group(1).strip().upper()
        
        if content == 'NONE':
            return cleaned_answer, []
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø¹Ø¯Ø§Ø¯
        try:
            indices = [int(x.strip()) for x in content.split(',') if x.strip().isdigit()]
            return cleaned_answer, indices
        except ValueError:
            logger.warning(f"Could not parse USED_SOURCES content: {content}")
            return cleaned_answer, None
    
    def _get_vector_field(self, dim: int) -> str:
        """Get vector field name based on dimension."""
        if dim <= 512:
            return "small"
        elif dim <= 768:
            return "medium"
        elif dim <= 1024:
            return "large"  # e5-large, bge-m3
        elif dim <= 1536:
            return "xlarge"  # OpenAI ada-002, text-embedding-3-small
        else:
            return "default"  # 3072
    
    async def _check_cache(self, query: RAGQuery) -> Optional[RAGResponse]:
        """
        Check if query result is cached.
        
        Args:
            query: Query to check
            
        Returns:
            Cached response if available
        """
        try:
            redis = await get_redis_client()
            
            # Generate cache key
            cache_key = self._generate_cache_key(query)
            
            # Check Redis cache
            cached = await redis.get(cache_key)
            if cached:
                data = json.loads(cached)
                
                # Reconstruct response
                chunks = [
                    RAGChunk(**chunk) for chunk in data["chunks"]
                ]
                
                return RAGResponse(
                    answer=data["answer"],
                    chunks=chunks,
                    sources=data["sources"],
                    total_tokens=data["total_tokens"],
                    processing_time_ms=data["processing_time_ms"],
                    cached=True,
                    model_used=data.get("model_used", ""),
                    input_tokens=data.get("input_tokens", 0),
                    output_tokens=data.get("output_tokens", 0)
                )
            
        except Exception as e:
            logger.warning(f"Cache check failed: {e}")
        
        return None
    
    async def _cache_response(
        self,
        query: RAGQuery,
        response: RAGResponse
    ):
        """
        Cache query response.
        
        Args:
            query: Original query
            response: Generated response
        """
        try:
            redis = await get_redis_client()
            
            # Generate cache key
            cache_key = self._generate_cache_key(query)
            
            # Prepare data for caching
            cache_data = {
                "answer": response.answer,
                "chunks": [
                    {
                        "text": c.text,
                        "score": c.score,
                        "source": c.source,
                        "metadata": c.metadata,
                        "document_id": c.document_id
                    }
                    for c in response.chunks
                ],
                "sources": response.sources,
                "total_tokens": response.total_tokens,
                "processing_time_ms": response.processing_time_ms,
                "model_used": response.model_used,
                "input_tokens": response.input_tokens,
                "output_tokens": response.output_tokens
            }
            
            # Cache in Redis with TTL
            await redis.setex(
                cache_key,
                settings.cache_ttl_query,
                json.dumps(cache_data, ensure_ascii=False)
            )
            
        except Exception as e:
            logger.warning(f"Cache save failed: {e}")
    
    def _generate_cache_key(self, query: RAGQuery) -> str:
        """Generate cache key for query."""
        # Create a unique key based on query parameters
        key_parts = [
            query.text.lower(),
            query.language,
            str(query.max_chunks),
            str(query.filters) if query.filters else "",
        ]
        
        key_string = "|".join(key_parts)
        key_hash = hashlib.md5(key_string.encode()).hexdigest()
        
        return f"rag:cache:{key_hash}"
