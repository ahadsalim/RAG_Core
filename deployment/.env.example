# =============================================================================
# Core System Environment Variables
# Copy this file to .env and fill in the values
# =============================================================================

# Application
APP_NAME="RAG Core System"
APP_VERSION="1.0.0"
ENVIRONMENT="development"
DEBUG=true
LOG_LEVEL="INFO"

# Server
HOST="0.0.0.0"
PORT=7001
WORKERS=4
RELOAD=true
DOMAIN_NAME=""

# Security - Internal Use Only
# این تنظیمات فقط برای سیستم Core است و نباید با سیستم‌های دیگر به اشتراک گذاشته شود

# Database - Core DB
# For Docker: use postgres-core:5432
# For local development: use localhost:7433
POSTGRES_DB=core_db
POSTGRES_USER=core_user
POSTGRES_PASSWORD=core_pass
DATABASE_URL="postgresql+asyncpg://core_user:core_pass@postgres-core:5432/core_db"
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=40
DATABASE_POOL_TIMEOUT=30
DATABASE_ECHO=false

# Qdrant Vector Database
# For Docker: use qdrant:6333
# For local development: use localhost:7333
QDRANT_HOST="qdrant"
QDRANT_PORT=6333
QDRANT_GRPC_PORT=7334
QDRANT_API_KEY=""
QDRANT_COLLECTION="legal_documents"
QDRANT_USE_GRPC=false

# Redis
# For Docker: use redis-core:6379
# For local development: use localhost:7379
REDIS_URL="redis://redis-core:6379/0"
REDIS_PASSWORD=""
REDIS_MAX_CONNECTIONS=50
REDIS_DECODE_RESPONSES=true

# Cache Settings
CACHE_TTL_DEFAULT=3600
CACHE_TTL_QUERY=7200
CACHE_TTL_EMBEDDING=86400
SEMANTIC_CACHE_THRESHOLD=0.95

# Celery
# For Docker: use redis-core:6379
# For local development: use localhost:7379
CELERY_BROKER_URL="redis://redis-core:6379/1"
CELERY_RESULT_BACKEND="redis://redis-core:6379/2"
CELERY_TASK_SERIALIZER="json"
CELERY_RESULT_SERIALIZER="json"
CELERY_TIMEZONE="Asia/Tehran"
CELERY_ENABLE_UTC=false

# ===========================================================================
# LLM Configuration (OpenAI-Compatible API)
# ===========================================================================
# برای OpenAI:
#   LLM_BASE_URL="" (خالی بگذارید)
#   LLM_API_KEY="sk-..."
#
# برای Local (LM Studio, LocalAI):
#   LLM_BASE_URL="http://localhost:1234/v1"
#   LLM_API_KEY="not-needed"

# ===========================================================================
# LLM1 (Light) - برای سوالات عمومی: invalid, general
# ===========================================================================
# استفاده برای: invalid_no_file, invalid_with_file, general
LLM1_API_KEY=""
LLM1_BASE_URL="https://api.openai.com/v1"
LLM1_MODEL="gpt-4o-mini"
LLM1_MAX_TOKENS=2048
LLM1_TEMPERATURE=0.7

# Fallback برای LLM1
LLM1_FALLBACK_API_KEY=""
LLM1_FALLBACK_BASE_URL=""
LLM1_FALLBACK_MODEL="gpt-4o-mini"

# ===========================================================================
# LLM2 (Pro) - برای سوالات کسب‌وکار: business
# ===========================================================================
# استفاده برای: business_no_file, business_with_file
LLM2_API_KEY=""
LLM2_BASE_URL="https://api.openai.com/v1"
LLM2_MODEL="gpt-4o"
LLM2_MAX_TOKENS=8192
LLM2_TEMPERATURE=0.4

# Fallback برای LLM2
LLM2_FALLBACK_API_KEY=""
LLM2_FALLBACK_BASE_URL=""
LLM2_FALLBACK_MODEL="gpt-4o"

# --- LLM Timeout Settings ---
# زمان انتظار برای LLM (ثانیه) - اگر primary جواب نداد، به fallback می‌رود
LLM_PRIMARY_TIMEOUT=30
# زمان انتظار برای web search (ثانیه) - بیشتر به دلیل فراخوانی API خارجی
LLM_WEB_SEARCH_TIMEOUT=90

# LLM Classification (for query categorization)
# این LLM برای دسته‌بندی سوالات استفاده می‌شود
LLM_CLASSIFICATION_API_KEY=""
LLM_CLASSIFICATION_BASE_URL="https://api.openai.com/v1"
LLM_CLASSIFICATION_MODEL="gpt-4o-mini"
LLM_CLASSIFICATION_MAX_TOKENS=512
LLM_CLASSIFICATION_TEMPERATURE=0.2

# Fallback Classification LLM
LLM_CLASSIFICATION_FALLBACK_API_KEY=""
LLM_CLASSIFICATION_FALLBACK_BASE_URL=""
LLM_CLASSIFICATION_FALLBACK_MODEL=""

# ===========================================================================
# Embedding Configuration
# ===========================================================================
# مدل Embedding برای تبدیل متن به بردار
# توجه: تغییر مدل نیازمند re-embed کردن تمام داده‌ها است
#
# مدل‌های پیشنهادی:
#   - intfloat/multilingual-e5-large (1024d) - توصیه شده ⭐
#   - intfloat/multilingual-e5-base (768d)
#   - BAAI/bge-m3 (1024d)
#
# اگر از API استفاده می‌کنید، EMBEDDING_API_KEY و BASE_URL را پر کنید
# در غیر این صورت خالی بگذارید تا از مدل Local استفاده شود

EMBEDDING_MODEL="intfloat/multilingual-e5-large"
EMBEDDING_DIM=1024
EMBEDDING_API_KEY=""
EMBEDDING_BASE_URL=""

# ===========================================================================
# Optional Services
# ===========================================================================

# Reranking (Local Docker Service)
# سرویس محلی reranker با مدل BAAI/bge-reranker-v2-m3
# این سرویس در یک container مستقل اجرا می‌شود
RERANKER_SERVICE_URL="http://reranker:8100"
RERANKING_MODEL="BAAI/bge-reranker-v2-m3"
RERANKING_TOP_K=10
# برای استفاده از Cohere API به جای سرویس محلی:
#   COHERE_API_KEY="your-key"
#   RERANKING_MODEL="rerank-multilingual-v2.0"
COHERE_API_KEY=""

# OCR Settings
OCR_LANGUAGE="fas+eng"
TESSERACT_CMD="/usr/bin/tesseract"
MAX_IMAGE_SIZE_MB=10

# RAG Settings
RAG_CHUNK_SIZE=450
RAG_CHUNK_OVERLAP=50
RAG_TOP_K_RETRIEVAL=20

# --- RAG Retrieval Settings ---
# تعداد chunks نهایی که به LLM داده می‌شود (پیش‌فرض API: 5، حداکثر: 20)
RAG_MAX_CHUNKS=5

# ضریب برای تعداد chunks اولیه از vector search (قبل از reranking)
# مثال: اگر MAX_CHUNKS=5 و RETRIEVE_MULTIPLIER=3 → 15 chunk از vector search گرفته می‌شود
RAG_RETRIEVE_MULTIPLIER=10

# --- Reranker Settings ---
# حداقل امتیاز reranker برای نگه داشتن chunk (0.0 تا 1.0)
# chunks با امتیاز کمتر از این حذف می‌شوند حتی اگر در top_k باشند
# مقدار 0.0 = بدون فیلتر (همه top_k نگه داشته می‌شوند)
# پیشنهاد: 0.3 برای فیلتر منابع بی‌ربط
RAG_RERANKER_THRESHOLD=0.3

RAG_TOP_K_RERANK=5
RAG_SIMILARITY_THRESHOLD=0.5
RAG_MAX_CONTEXT_LENGTH=8192
RAG_USE_HYBRID_SEARCH=true
RAG_BM25_WEIGHT=0.3
RAG_VECTOR_WEIGHT=0.7

# تعیین پیش فرض RAG Web Search - غیرفعال برای کاهش زمان پاسخ‌دهی (منابع RAG کافی هستند)
ENABLE_RAG_WEB_SEARCH=false

# Search Settings
SEARCH_MAX_RESULTS=50
SEARCH_TIMEOUT=30
FUZZY_MATCH_THRESHOLD=0.8

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_PER_MINUTE=60
RATE_LIMIT_PER_HOUR=1000
RATE_LIMIT_PER_DAY=10000

# ===========================================================================
# MinIO (External Object Storage) - REQUIRED FOR PRODUCTION
# Fill these values from your MinIO server
# ===========================================================================
S3_ENDPOINT_URL=""
S3_ACCESS_KEY_ID=""
S3_SECRET_ACCESS_KEY=""
S3_REGION="us-east-1"
S3_USE_SSL=true

# MinIO Buckets
# ingest-system: اسناد و قوانین از سیستم Ingest
# temp-userfile: فایل‌های موقت کاربران از سیستم Users
S3_DOCUMENTS_BUCKET="ingest-system"
S3_TEMP_BUCKET="temp-userfile"

# ===========================================================================
# External Systems Integration - IMPORTANT!
# ===========================================================================
# این بخش شامل اطلاعاتی است که باید با سیستم‌های Ingest و Users به اشتراک گذاشته شود
# این مقادیر را در فایل .env سیستم‌های Ingest و Users نیز قرار دهید

# --- JWT Configuration (SHARED) ---
# این کلید JWT باید در هر سه سیستم (Core, Ingest, Users) یکسان باشد
# تا توکن‌های صادر شده توسط یک سیستم، توسط سیستم‌های دیگر قابل تایید باشد
# توجه: این کلید توسط اسکریپت start.sh به صورت خودکار تولید می‌شود
JWT_SECRET_KEY="your-jwt-secret-key-change-in-production"
JWT_ALGORITHM="HS256"
ACCESS_TOKEN_EXPIRE_MINUTES=30
REFRESH_TOKEN_EXPIRE_DAYS=7

# --- API Keys for Inter-Service Communication ---
# INGEST_API_KEY: سیستم Ingest از این کلید برای فراخوانی APIهای Core استفاده می‌کند
#                 این کلید را در .env سیستم Ingest نیز قرار دهید
# USERS_API_KEY:  سیستم Users از این کلید برای فراخوانی APIهای Core استفاده می‌کند
#                 این کلید را در .env سیستم Users نیز قرار دهید
# توجه: این کلیدها توسط اسکریپت start.sh به صورت خودکار تولید می‌شوند
INGEST_API_KEY=""
USERS_API_KEY=""

# System Limits
MAX_CONCURRENT_REQUESTS=100
MAX_HISTORY_LENGTH=50
MAX_QUERY_LENGTH=2000
REQUEST_TIMEOUT=60

# Temporary File Storage
# مدت زمان نگهداری فایل‌های موقت کاربران (ساعت) - بعد از این زمان فایل‌ها از S3 حذف می‌شوند
# تحلیل فایل در حافظه گفتگو باقی می‌ماند
TEMP_FILE_EXPIRATION_HOURS=12

# ===========================================================================
# Backup Server Configuration (for automatic backups)
# ===========================================================================
# سرور backup برای ارسال خودکار backup ها
# Run: ./deployment/backup_auto.sh setup
BACKUP_SERVER_HOST=""
BACKUP_SERVER_USER=""
BACKUP_SERVER_PATH="/backups/core"
BACKUP_SSH_KEY="/root/.ssh/backup_key"
BACKUP_RETENTION_DAYS=30
BACKUP_KEEP_LOCAL=false
