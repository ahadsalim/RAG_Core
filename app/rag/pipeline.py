"""
RAG Pipeline
Complete Retrieval-Augmented Generation pipeline
"""

from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import asyncio
import hashlib
from datetime import datetime, timedelta

import structlog
from tenacity import retry, stop_after_attempt, wait_exponential

from app.services.qdrant_service import QdrantService
from app.services.embedding_service import get_embedding_service  # Unified embedding service
from app.llm.openai_provider import OpenAIProvider
from app.llm.base import Message, LLMConfig
from app.llm.classifier import QueryClassifier
from app.llm.factory import create_llm2_pro  # LLM2 (Pro) Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±
from app.core.dependencies import get_redis_client
from app.config.settings import settings

logger = structlog.get_logger()


@dataclass
class RAGQuery:
    """RAG query request."""
    text: str
    user_id: str
    conversation_id: Optional[str] = None
    language: str = "fa"
    max_chunks: int = 5
    filters: Optional[Dict[str, Any]] = None
    use_cache: bool = True
    use_reranking: bool = True
    user_preferences: Optional[Dict[str, Any]] = None


@dataclass
class RAGChunk:
    """Retrieved document chunk."""
    text: str
    score: float
    source: str
    metadata: Dict[str, Any]
    document_id: Optional[str] = None


@dataclass
class RAGResponse:
    """RAG pipeline response."""
    answer: str
    chunks: List[RAGChunk]
    sources: List[str]
    total_tokens: int
    processing_time_ms: int
    cached: bool = False
    model_used: str = ""


class RAGPipeline:
    """Complete RAG pipeline for question answering."""
    
    def __init__(self):
        """Initialize RAG pipeline components."""
        self.qdrant = QdrantService()
        # Use unified embedding service (auto-detects API vs local)
        self.embedder = get_embedding_service()
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LLM2 (Pro) Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±
        self.llm = create_llm2_pro()
        self.classifier = QueryClassifier()  # LLM Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø³ÙˆØ§Ù„Ø§Øª
        self.reranker = None  # Will be initialized if needed
        logger.info("RAG Pipeline initialized with LLM2 (Pro)")
        
    async def process(self, query: RAGQuery, additional_context: str = None) -> RAGResponse:
        """
        Process a query through the RAG pipeline.
        
        Args:
            query: RAG query request
            additional_context: Additional context for LLM (memory, file analysis, etc.)
            
        Returns:
            RAG response with answer and sources
        """
        start_time = datetime.utcnow()
        
        try:
            # Step 0: Classify query using LLM (if enabled)
            classification = None
            if settings.enable_query_classification:
                classification = await self.classifier.classify(query.text, query.language)
                
                logger.info(
                    "Query classified",
                    category=classification.category,
                    confidence=classification.confidence
                )
            
            # Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ø§Ø­ÙˆØ§Ù„Ù¾Ø±Ø³ÛŒØŒ Ú†Ø±Øªâ€ŒÙˆÙ¾Ø±ØªØŒ ÛŒØ§ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨ÙˆØ¯ â†’ Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ…
            if classification and classification.category != "business_question":
                processing_time = int((datetime.utcnow() - start_time).total_seconds() * 1000)
                
                return RAGResponse(
                    answer=classification.direct_response or "Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¨Ù‡ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ù¾Ø§Ø³Ø® Ø¯Ù‡Ù….",
                    chunks=[],
                    sources=[],
                    total_tokens=0,
                    processing_time_ms=processing_time,
                    cached=False,
                    model_used=self.classifier.llm_config.model
                )
            
            # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
            
            # Check cache if enabled
            if query.use_cache:
                cached_response = await self._check_cache(query)
                if cached_response:
                    cached_response.cached = True
                    return cached_response
            
            # Step 1: Query understanding and enhancement
            enhanced_query = await self._enhance_query(query)
            
            # Step 2: Generate embedding
            query_embedding = await self._generate_embedding(enhanced_query)
            
            # Step 3: Retrieve relevant chunks
            chunks = await self._retrieve_chunks(
                query_embedding,
                enhanced_query,
                query.filters,
                limit=query.max_chunks * 3  # Get more for reranking
            )
            
            logger.info(
                "Retrieved chunks",
                query=query.text[:100],
                enhanced_query=enhanced_query[:100],
                num_chunks=len(chunks),
                top_scores=[c.score for c in chunks[:3]] if chunks else []
            )
            
            # Step 4: Rerank if enabled
            if query.use_reranking and len(chunks) > query.max_chunks:
                chunks = await self._rerank_chunks(
                    enhanced_query,
                    chunks,
                    top_k=query.max_chunks
                )
                logger.info(
                    "Reranked chunks",
                    final_count=len(chunks),
                    top_scores=[c.score for c in chunks[:3]] if chunks else []
                )
            else:
                chunks = chunks[:query.max_chunks]
                logger.info(
                    "Using top chunks without reranking",
                    count=len(chunks)
                )
            
            # Step 5: Generate answer
            logger.info(
                "Generating answer",
                num_chunks=len(chunks),
                chunk_sources=[c.metadata.get('work_title', 'N/A')[:50] for c in chunks[:3]]
            )
            
            answer, tokens_used = await self._generate_answer(
                query.text,
                chunks,
                query.language,
                query.conversation_id,
                query.user_preferences,
                additional_context=additional_context
            )
            
            # Step 6: Extract sources
            sources = self._extract_sources(chunks)
            
            # Calculate processing time
            processing_time = int(
                (datetime.utcnow() - start_time).total_seconds() * 1000
            )
            
            # Create response
            response = RAGResponse(
                answer=answer,
                chunks=chunks,
                sources=sources,
                total_tokens=tokens_used,
                processing_time_ms=processing_time,
                model_used=self.llm.config.model
            )
            
            # Cache response if enabled
            if query.use_cache:
                await self._cache_response(query, response)
            
            return response
            
        except Exception as e:
            logger.error(f"RAG pipeline error: {e}")
            raise
    
    async def _enhance_query(self, query: RAGQuery) -> str:
        """
        Enhance query for better retrieval using LLM.
        
        Args:
            query: Original query
            
        Returns:
            Enhanced query text
        """
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LLM Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ query
        try:
            if query.language == "fa":
                system_prompt = """Ø´Ù…Ø§ ÛŒÚ© Ù…ØªØ®ØµØµ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø³Ù†Ø§Ø¯ Ø­Ù‚ÙˆÙ‚ÛŒ Ù‡Ø³ØªÛŒØ¯.
ÙˆØ¸ÛŒÙÙ‡ Ø´Ù…Ø§: Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±Ø¯Ù‡ Ùˆ Ø¢Ù† Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ú©Ù†ÛŒØ¯.

Ú©Ø§Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯:
1. Ø§Ø®ØªØµØ§Ø±Ø§Øª Ù‚ÙˆØ§Ù†ÛŒÙ† Ø±Ø§ Ø¨Ø§Ø² Ú©Ù†ÛŒØ¯ (Ù…Ø«Ù„ Ù‚.Ù… â†’ Ù‚Ø§Ù†ÙˆÙ† Ù…Ø¯Ù†ÛŒØŒ Ù‚.Øª.Ø§ â†’ Ù‚Ø§Ù†ÙˆÙ† ØªØ£Ù…ÛŒÙ† Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ)
2. Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯ (Û±Û²Û³ â†’ 123)
3. Ø§Ø¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ù…ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯ (Ø¯Ù‡ â†’ 10ØŒ Ø¨ÛŒØ³Øª Ùˆ Ù¾Ù†Ø¬ â†’ 25)
4. Ø§Ù…Ù„Ø§ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡ Ø±Ø§ ØªØµØ­ÛŒØ­ Ú©Ù†ÛŒØ¯
5. Ú©Ù„Ù…Ø§Øª Ù…ØªØ±Ø§Ø¯Ù Ù…Ù‡Ù… Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯ (Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²)

Ù…Ù‡Ù…: ÙÙ‚Ø· query Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯ØŒ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­ Ø§Ø¶Ø§ÙÛŒ.

Ù…Ø«Ø§Ù„ 1:
ÙˆØ±ÙˆØ¯ÛŒ: "Ù‚.Ù… Ù…Ø§Ø¯Ù‡ Û±Û·Û¹"
Ø®Ø±ÙˆØ¬ÛŒ: "Ù‚Ø§Ù†ÙˆÙ† Ù…Ø¯Ù†ÛŒ Ù…Ø§Ø¯Ù‡ 179"

Ù…Ø«Ø§Ù„ 2:
ÙˆØ±ÙˆØ¯ÛŒ: "Ù…Ø§Ø¯Ù‡ Ø¯Ù‡ Ù‚Ø§Ù†ÙˆÙ† Ú†Ù„Ù…Ù†Ú¯Ø§Ù†"
Ø®Ø±ÙˆØ¬ÛŒ: "Ù…Ø§Ø¯Ù‡ 10 Ù‚Ø§Ù†ÙˆÙ† Ú†Ù„Ù…Ù†Ú¯Ø§Ù†"

Ù…Ø«Ø§Ù„ 3:
ÙˆØ±ÙˆØ¯ÛŒ: "Ù‚.Øª.Ø§ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø¨Ø§Ø²Ù†Ø´Ø³ØªÚ¯ÛŒ"
Ø®Ø±ÙˆØ¬ÛŒ: "Ù‚Ø§Ù†ÙˆÙ† ØªØ£Ù…ÛŒÙ† Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ø¨Ø§Ø²Ù†Ø´Ø³ØªÚ¯ÛŒ"

ÙÙ‚Ø· query Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯."""

                user_message = f"Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {query.text}"
                
                messages = [
                    Message(role="system", content=system_prompt),
                    Message(role="user", content=user_message)
                ]
                
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LLM Ø³Ø¨Ú©â€ŒØªØ± Ø¨Ø±Ø§ÛŒ enhancement
                response = await self.llm.generate(
                    messages,
                    temperature=0.1,  # Ú©Ù… Ø¨Ø±Ø§ÛŒ consistency
                    max_tokens=200
                )
                
                enhanced = response.content.strip()
                
                # Ø§Ú¯Ø± LLM Ú†ÛŒØ² Ø¹Ø¬ÛŒØ¨ÛŒ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯ØŒ Ø§Ø² query Ø§ØµÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
                if not enhanced or len(enhanced) > len(query.text) * 3:
                    enhanced = query.text
                    logger.warning("LLM enhancement failed, using original query")
                
                logger.info(f"Query enhanced via LLM: '{query.text}' -> '{enhanced}'")
                return enhanced
                
            else:
                # Ø¨Ø±Ø§ÛŒ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± ÙØ¹Ù„Ø§Ù‹ enhancement Ù†Ø¯Ø§Ø±ÛŒÙ…
                return query.text
                
        except Exception as e:
            logger.warning(f"Query enhancement failed: {e}, using original query")
            return query.text
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    async def _generate_embedding(self, text: str) -> List[float]:
        """
        Generate embedding for text with retry logic.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector
        """
        # Local embedding is synchronous, wrap it in async
        import asyncio
        loop = asyncio.get_event_loop()
        embedding = await loop.run_in_executor(
            None, 
            self.embedder.encode_single,
            text
        )
        return embedding.tolist()
    
    async def _retrieve_chunks(
        self,
        query_embedding: List[float],
        query_text: str,
        filters: Optional[Dict[str, Any]],
        limit: int
    ) -> List[RAGChunk]:
        """
        Retrieve relevant chunks from vector database.
        
        Args:
            query_embedding: Query embedding vector
            query_text: Query text for hybrid search
            filters: Optional filters
            limit: Maximum chunks to retrieve
            
        Returns:
            List of relevant chunks
        """
        # Determine vector field based on embedding dimension
        vector_field = self._get_vector_field(len(query_embedding))
        
        # Perform hybrid search if enabled
        if settings.rag_use_hybrid_search:
            results = await self.qdrant.hybrid_search(
                query_vector=query_embedding,
                query_text=query_text,
                limit=limit,
                vector_weight=settings.rag_vector_weight,
                keyword_weight=settings.rag_bm25_weight,
                filters=filters,
                vector_field=vector_field
            )
        else:
            # Vector-only search
            results = await self.qdrant.search(
                query_vector=query_embedding,
                limit=limit,
                score_threshold=settings.rag_similarity_threshold,
                filters=filters,
                vector_field=vector_field
            )
        
        # Convert to RAGChunk objects
        chunks = []
        for result in results:
            chunk = RAGChunk(
                text=result["text"],
                score=result.get("score", 0.0),
                source=result.get("source", "unknown"),
                metadata=result.get("metadata", {}),
                document_id=result.get("document_id")
            )
            chunks.append(chunk)
        
        logger.debug(
            "Retrieved chunks from Qdrant",
            count=len(chunks),
            vector_field=vector_field,
            top_3_docs=[c.metadata.get('work_title', 'N/A')[:30] for c in chunks[:3]]
        )
        
        return chunks
    
    async def _rerank_chunks(
        self,
        query: str,
        chunks: List[RAGChunk],
        top_k: int
    ) -> List[RAGChunk]:
        """
        Rerank chunks for better relevance.
        
        Args:
            query: Query text
            chunks: Retrieved chunks
            top_k: Number of top chunks to return
            
        Returns:
            Reranked chunks
        """
        if not chunks:
            return []
        
        # If we have Cohere reranker configured
        if settings.cohere_api_key and self.reranker:
            try:
                reranked = await self.reranker.rerank(
                    query=query,
                    documents=[c.text for c in chunks],
                    top_k=top_k
                )
                
                # Reorder chunks based on reranking
                reranked_chunks = []
                for idx, score in reranked:
                    chunk = chunks[idx]
                    chunk.score = score  # Update score with rerank score
                    reranked_chunks.append(chunk)
                
                return reranked_chunks
                
            except Exception as e:
                logger.warning(f"Reranking failed, using original order: {e}")
        
        # Fallback: Simple score-based reranking
        # Combine original score with text similarity
        return sorted(chunks, key=lambda x: x.score, reverse=True)[:top_k]
    
    async def _generate_answer(
        self,
        query: str,
        chunks: List[RAGChunk],
        language: str,
        conversation_id: Optional[str],
        user_preferences: Optional[Dict[str, Any]] = None,
        additional_context: str = None
    ) -> Tuple[str, int]:
        """
        Generate answer using LLM with retrieved context.
        
        Args:
            query: User query
            chunks: Retrieved chunks
            language: Response language
            conversation_id: Optional conversation ID for context
            user_preferences: Optional user preferences for response customization
            additional_context: Additional context (memory, file analysis, etc.)
            
        Returns:
            Generated answer and tokens used
        """
        # Build context from chunks
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            source_info = f"[Ù…Ù†Ø¨Ø¹ {i}]"
            work_title = chunk.metadata.get("work_title") or chunk.metadata.get("document_title")
            if work_title:
                source_info += f" {work_title}"
            if chunk.metadata.get("unit_number"):
                source_info += f" - Ù…Ø§Ø¯Ù‡ {chunk.metadata['unit_number']}"
            
            context_parts.append(f"{source_info}:\n{chunk.text}")
        
        context = "\n\n".join(context_parts)
        
        # Build system prompt
        system_prompt = self._build_system_prompt(language, user_preferences)
        
        # Build user message
        if language == "fa":
            user_message_parts = []
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† additional context (Ø­Ø§ÙØ¸Ù‡ØŒ ÙØ§ÛŒÙ„ØŒ Ùˆ...)
            if additional_context:
                user_message_parts.append(additional_context)
                user_message_parts.append("\n" + "="*50 + "\n")
            
            user_message_parts.append(f"""Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±Ø¬Ø¹ Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡:
{context}""")
            
            user_message = "\n".join(user_message_parts)
        else:
            user_message_parts = []
            
            if additional_context:
                user_message_parts.append(additional_context)
                user_message_parts.append("\n" + "="*50 + "\n")
            
            user_message_parts.append(f"""Reference information from database:
{context}""")
            
            user_message = "\n".join(user_message_parts)
        
        # Add user preferences to the message if provided
        if user_preferences:
            prefs_text = self._format_user_preferences(user_preferences, language)
            if prefs_text:
                user_message += f"\n\n{prefs_text}"
        
        # Build messages
        messages = [
            Message(role="system", content=system_prompt),
            Message(role="user", content=user_message)
        ]
        
        # Add conversation history if available
        if conversation_id:
            # TODO: Load conversation history from database
            pass
        
        # Generate response
        response = await self.llm.generate(messages)
        
        return response.content, response.usage["total_tokens"]
    
    def _build_system_prompt(self, language: str, user_preferences: Optional[Dict[str, Any]] = None) -> str:
        """Build system prompt based on language and user preferences."""
        from datetime import datetime
        import pytz
        import jdatetime
        from app.config.prompts import RAGPrompts
        
        # Get current date and time in Tehran timezone
        tehran_tz = pytz.timezone('Asia/Tehran')
        now = datetime.now(tehran_tz)
        
        # Convert to Jalali (Shamsi) calendar
        jalali_now = jdatetime.datetime.fromgregorian(datetime=now)
        current_date_shamsi = jalali_now.strftime('%Y/%m/%d')  # 1404/09/10
        current_time_fa = now.strftime('%H:%M')     # 16:24
        
        if language == "fa":
            base_prompt = RAGPrompts.get_rag_system_prompt_fa(
                current_date_shamsi=current_date_shamsi,
                current_time_fa=current_time_fa
            )
        else:
            # English prompt
            current_date_gregorian = now.strftime('%Y-%m-%d')
            current_time_en = now.strftime('%H:%M')
            
            base_prompt = RAGPrompts.get_rag_system_prompt_en(
                current_date_gregorian=current_date_gregorian,
                current_date_shamsi=current_date_shamsi,
                current_time=current_time_en
            )
        
        # Add user preferences to system prompt if provided
        if user_preferences:
            pref_additions = []
            
            if user_preferences.get("response_style"):
                style = user_preferences["response_style"]
                if language == "fa":
                    pref_additions.append(f"- Ø³Ø¨Ú© Ù¾Ø§Ø³Ø®: {style}")
                else:
                    pref_additions.append(f"- Response style: {style}")
            
            if user_preferences.get("detail_level"):
                level = user_preferences["detail_level"]
                if language == "fa":
                    pref_additions.append(f"- Ø³Ø·Ø­ Ø¬Ø²Ø¦ÛŒØ§Øª: {level}")
                else:
                    pref_additions.append(f"- Detail level: {level}")
            
            if pref_additions:
                if language == "fa":
                    base_prompt += "\n\nØªØ±Ø¬ÛŒØ­Ø§Øª Ú©Ø§Ø±Ø¨Ø±:\n" + "\n".join(pref_additions)
                else:
                    base_prompt += "\n\nUser preferences:\n" + "\n".join(pref_additions)
        
        return base_prompt
    
    def _format_user_preferences(self, preferences: Dict[str, Any], language: str) -> str:
        """Format user preferences into a readable instruction for LLM."""
        if not preferences:
            return ""
        
        instructions = []
        
        if language == "fa":
            if preferences.get("response_style"):
                style_map = {
                    "formal": "Ø±Ø³Ù…ÛŒ Ùˆ ØªØ®ØµØµÛŒ",
                    "casual": "ØºÛŒØ±Ø±Ø³Ù…ÛŒ Ùˆ Ø³Ø§Ø¯Ù‡",
                    "academic": "Ø¢Ú©Ø§Ø¯Ù…ÛŒÚ© Ùˆ Ø¹Ù„Ù…ÛŒ",
                    "simple": "Ø³Ø§Ø¯Ù‡ Ùˆ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù…"
                }
                style = style_map.get(preferences["response_style"], preferences["response_style"])
                instructions.append(f"Ø³Ø¨Ú© Ù¾Ø§Ø³Ø®: {style}")
            
            if preferences.get("detail_level"):
                level_map = {
                    "brief": "Ø®Ù„Ø§ØµÙ‡ Ùˆ Ù…Ø®ØªØµØ±",
                    "moderate": "Ù…ØªÙˆØ³Ø·",
                    "comprehensive": "Ø¬Ø§Ù…Ø¹ Ùˆ Ú©Ø§Ù…Ù„",
                    "detailed": "Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ú©Ø§Ù…Ù„"
                }
                level = level_map.get(preferences["detail_level"], preferences["detail_level"])
                instructions.append(f"Ø³Ø·Ø­ Ø¬Ø²Ø¦ÛŒØ§Øª: {level}")
            
            if preferences.get("include_examples"):
                if preferences["include_examples"]:
                    instructions.append("Ù„Ø·ÙØ§Ù‹ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯")
            
            if preferences.get("language_style"):
                style_map = {
                    "simple": "Ø§Ø² Ø²Ø¨Ø§Ù† Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯",
                    "technical": "Ø§Ø² Ø§ØµØ·Ù„Ø§Ø­Ø§Øª ØªØ®ØµØµÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯",
                    "mixed": "ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ø²Ø¨Ø§Ù† Ø³Ø§Ø¯Ù‡ Ùˆ ØªØ®ØµØµÛŒ"
                }
                style = style_map.get(preferences["language_style"], preferences["language_style"])
                instructions.append(style)
            
            if preferences.get("format"):
                format_map = {
                    "bullet_points": "Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯",
                    "numbered_list": "Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù„ÛŒØ³Øª Ø´Ù…Ø§Ø±Ù‡â€ŒØ¯Ø§Ø± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯",
                    "paragraph": "Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø³Ø¬Ù… Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯"
                }
                fmt = format_map.get(preferences["format"], preferences["format"])
                instructions.append(fmt)
            
            if instructions:
                return "Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù¾Ø§Ø³Ø®:\n" + "\n".join(f"- {inst}" for inst in instructions)
        
        else:  # English
            if preferences.get("response_style"):
                instructions.append(f"Response style: {preferences['response_style']}")
            
            if preferences.get("detail_level"):
                instructions.append(f"Detail level: {preferences['detail_level']}")
            
            if preferences.get("include_examples") and preferences["include_examples"]:
                instructions.append("Please include practical examples")
            
            if preferences.get("language_style"):
                instructions.append(f"Language style: {preferences['language_style']}")
            
            if preferences.get("format"):
                instructions.append(f"Format: {preferences['format']}")
            
            if instructions:
                return "Response guidelines:\n" + "\n".join(f"- {inst}" for inst in instructions)
        
        return ""
    
    def _extract_sources(self, chunks: List[RAGChunk]) -> List[str]:
        """Extract detailed sources from chunks with full context."""
        sources = []
        seen = set()
        
        for i, chunk in enumerate(chunks, 1):
            metadata = chunk.metadata
            source_lines = []
            
            # 1. Ø´Ù…Ø§Ø±Ù‡ Ù…Ù†Ø¨Ø¹ Ùˆ Ù…ØªÙ† Ú©Ø§Ù…Ù„
            source_lines.append(f"ðŸ“Œ Ù…Ù†Ø¨Ø¹ {i}:")
            source_lines.append(f"ðŸ“„ Ù…ØªÙ†: {chunk.text}")
            source_lines.append("")  # Ø®Ø· Ø®Ø§Ù„ÛŒ
            
            # 2. Ù†Ø§Ù… Ù‚Ø§Ù†ÙˆÙ†/Ø³Ù†Ø¯ Ùˆ Ù†ÙˆØ¹
            doc_type = metadata.get("document_type") or metadata.get("doc_type", "")
            doc_title = metadata.get("document_title", "")
            unit_type = metadata.get("unit_type", "")
            
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² work_title Ø¨Ù‡ Ø¬Ø§ÛŒ document_title
            work_title = metadata.get("work_title", "")
            if not work_title:
                work_title = doc_title
            
            if work_title:
                source_lines.append(f"ï¿½ Ù†Ø§Ù… Ø³Ù†Ø¯: {work_title}")
                if doc_type and doc_type != work_title:
                    source_lines.append(f"ðŸ“‹ Ù†ÙˆØ¹: {doc_type}")
            
            # 3. Ù…Ø³ÛŒØ± Ø¯Ù‚ÛŒÙ‚ (Ø§Ø² path_label ÛŒØ§ Ø³Ø§Ø®Øª Ø¯Ø³ØªÛŒ)
            path_label = metadata.get("path_label", "")
            
            if path_label:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø³ÛŒØ± Ú©Ø§Ù…Ù„ Ø§Ø² metadata
                source_lines.append(f"ðŸ“ Ù…Ø³ÛŒØ±: {path_label}")
            else:
                # Ø³Ø§Ø®Øª Ù…Ø³ÛŒØ± Ø§Ø² ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
                unit_number = metadata.get("unit_number")
                title = metadata.get("title", "")
                
                if unit_number:
                    if unit_type == "article":
                        source_lines.append(f"ðŸ“ Ù…Ø§Ø¯Ù‡ {unit_number}")
                    elif unit_type:
                        source_lines.append(f"ðŸ“ {unit_type} {unit_number}")
                    else:
                        source_lines.append(f"ðŸ“ Ù…Ø§Ø¯Ù‡ {unit_number}")
                
                if title and title != work_title:
                    source_lines.append(f"   Ø¹Ù†ÙˆØ§Ù†: {title}")
            
            # 4. Ù…Ø±Ø¬Ø¹ ØªØµÙˆÛŒØ¨ (ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ ØºÛŒØ± Ù‚ÙˆØ§Ù†ÛŒÙ†)
            authority = metadata.get("authority", "")
            
            # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø³Ù†Ø¯ - Ø§Ú¯Ø± Ù‚Ø§Ù†ÙˆÙ† Ø§Ø³ØªØŒ Ù…Ø±Ø¬Ø¹ ØªØµÙˆÛŒØ¨ Ù†Ù…Ø§ÛŒØ´ Ù†Ø¯Ù‡
            is_law = work_title and ("Ù‚Ø§Ù†ÙˆÙ†" in work_title.lower())
            
            # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø¨Ø®Ø´Ù†Ø§Ù…Ù‡/Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡/Ø±Ø§ÛŒ Ù…Ø±Ø¬Ø¹ ØªØµÙˆÛŒØ¨ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
            if authority and not is_law:
                source_lines.append(f"âœ… Ù…Ø±Ø¬Ø¹ ØªØµÙˆÛŒØ¨: {authority}")
            
            # Ø³Ø§Ø®Øª source Ù†Ù‡Ø§ÛŒÛŒ
            source = "\n".join(source_lines)
            
            # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ document_id + unit_number
            source_key = f"{metadata.get('document_id', '')}_{metadata.get('unit_number', '')}"
            if source_key not in seen:
                sources.append(source)
                seen.add(source_key)
        
        return sources
    
    def _get_vector_field(self, dim: int) -> str:
        """Get vector field name based on dimension."""
        if dim <= 512:
            return "small"
        elif dim <= 768:
            return "medium"
        elif dim <= 1024:
            return "large"  # e5-large, bge-m3
        elif dim <= 1536:
            return "xlarge"  # OpenAI ada-002, text-embedding-3-small
        else:
            return "default"  # 3072
    
    async def _check_cache(self, query: RAGQuery) -> Optional[RAGResponse]:
        """
        Check if query result is cached.
        
        Args:
            query: Query to check
            
        Returns:
            Cached response if available
        """
        try:
            redis = await get_redis_client()
            
            # Generate cache key
            cache_key = self._generate_cache_key(query)
            
            # Check Redis cache
            cached = await redis.get(cache_key)
            if cached:
                import json
                data = json.loads(cached)
                
                # Reconstruct response
                chunks = [
                    RAGChunk(**chunk) for chunk in data["chunks"]
                ]
                
                return RAGResponse(
                    answer=data["answer"],
                    chunks=chunks,
                    sources=data["sources"],
                    total_tokens=data["total_tokens"],
                    processing_time_ms=data["processing_time_ms"],
                    cached=True,
                    model_used=data.get("model_used", "")
                )
            
            # TODO: Check database cache for semantic similarity
            
        except Exception as e:
            logger.warning(f"Cache check failed: {e}")
        
        return None
    
    async def _cache_response(
        self,
        query: RAGQuery,
        response: RAGResponse
    ):
        """
        Cache query response.
        
        Args:
            query: Original query
            response: Generated response
        """
        try:
            redis = await get_redis_client()
            
            # Generate cache key
            cache_key = self._generate_cache_key(query)
            
            # Prepare data for caching
            import json
            cache_data = {
                "answer": response.answer,
                "chunks": [
                    {
                        "text": c.text,
                        "score": c.score,
                        "source": c.source,
                        "metadata": c.metadata,
                        "document_id": c.document_id
                    }
                    for c in response.chunks
                ],
                "sources": response.sources,
                "total_tokens": response.total_tokens,
                "processing_time_ms": response.processing_time_ms,
                "model_used": response.model_used
            }
            
            # Cache in Redis with TTL
            await redis.setex(
                cache_key,
                settings.cache_ttl_query,
                json.dumps(cache_data, ensure_ascii=False)
            )
            
            # TODO: Also cache in database for semantic search
            
        except Exception as e:
            logger.warning(f"Cache save failed: {e}")
    
    def _generate_cache_key(self, query: RAGQuery) -> str:
        """Generate cache key for query."""
        # Create a unique key based on query parameters
        key_parts = [
            query.text.lower(),
            query.language,
            str(query.max_chunks),
            str(query.filters) if query.filters else "",
        ]
        
        key_string = "|".join(key_parts)
        key_hash = hashlib.md5(key_string.encode()).hexdigest()
        
        return f"rag:cache:{key_hash}"
