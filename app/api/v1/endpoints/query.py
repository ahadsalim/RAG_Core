"""
Query Processing API Endpoints - Enhanced Version
Ù†Ø³Ø®Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„ØŒ Ø­Ø§ÙØ¸Ù‡ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª Ùˆ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª
"""

from typing import Optional, Dict, Any, List
from datetime import datetime
import uuid

from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks
from sqlalchemy.ext.asyncio import AsyncSession
from pydantic import BaseModel, Field
import structlog

from app.db.session import get_db
from app.rag.pipeline import RAGPipeline, RAGQuery
from app.models.user import UserProfile, Conversation, Message as DBMessage, MessageRole
from app.core.security import get_current_user_id
from app.config.settings import settings
from app.services.conversation_memory import get_conversation_memory, ConversationMemory
from app.services.long_term_memory import get_long_term_memory_service, LongTermMemoryService

# Import shared utilities
from app.api.v1.endpoints.query_utils import (
    get_current_shamsi_datetime,
    get_or_create_user,
    get_or_create_conversation,
    get_conversation_context,
    build_llm_context,
    process_file_attachments,
    save_conversation_messages,
    classify_query_with_context,
)

logger = structlog.get_logger()
router = APIRouter()

# ============================================================================
# DEBUG MODE - Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯ÛŒØ¨Ø§Ú¯ Ø¨Ù‡ Ø§Ø¨ØªØ¯Ø§ÛŒ Ù¾Ø§Ø³Ø® (Ù…ÙˆÙ‚Øª Ø¨Ø±Ø§ÛŒ ØªØ³Øª)
# ============================================================================
DEBUG_MODE = True  # Ø¨Ø±Ø§ÛŒ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù†ØŒ False Ú©Ù†ÛŒØ¯

def add_debug_info(
    answer: str,
    category: str,
    model: str,
    input_tokens: int = 0,
    output_tokens: int = 0,
    confidence: float = 0.0,
    cached: bool = False,
    reranker_details: list = None
) -> str:
    """
    Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯ÛŒØ¨Ø§Ú¯ Ø¨Ù‡ Ø§Ø¨ØªØ¯Ø§ÛŒ Ù¾Ø§Ø³Ø® (Ù…ÙˆÙ‚Øª Ø¨Ø±Ø§ÛŒ ØªØ³Øª)
    """
    if not DEBUG_MODE:
        return answer
    
    # Ø§Ú¯Ø± Ø§Ø² cache Ø¢Ù…Ø¯Ù‡ØŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ ØµÙØ± Ù‡Ø³ØªÙ†Ø¯ (Ù‡Ø²ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ…)
    if cached:
        token_info = "ğŸ’¾ Ø§Ø² Ú©Ø´ (Ø¨Ø¯ÙˆÙ† Ù‡Ø²ÛŒÙ†Ù‡ ØªÙˆÚ©Ù†)"
    else:
        token_info = f"ğŸ“¥ ØªÙˆÚ©Ù† ÙˆØ±ÙˆØ¯ÛŒ: `{input_tokens}` | ğŸ“¤ ØªÙˆÚ©Ù† Ø®Ø±ÙˆØ¬ÛŒ: `{output_tokens}`"
    
    # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ reranker
    reranker_info = ""
    if reranker_details:
        reranker_lines = ["\nğŸ”„ **Reranker Results (Ù‡Ù…Ù‡ chunks):**"]
        for i, detail in enumerate(reranker_details):
            score = detail.get("score", 0)
            source = detail.get("source", "?")[:40]
            unit = detail.get("unit", "")
            # Ù†Ø´Ø§Ù†Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ top 5 Ú©Ù‡ Ø¨Ù‡ LLM Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯
            marker = "âœ…" if i < 5 else "âŒ"
            reranker_lines.append(f"  {marker} #{i+1}: `{score:.4f}` | {source} | Ù…Ø§Ø¯Ù‡ {unit}")
        reranker_info = "\n".join(reranker_lines)
    
    debug_header = f"""ğŸ“Š **[DEBUG INFO]**
ğŸ·ï¸ Ø¯Ø³ØªÙ‡: `{category}` | Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: `{confidence:.0%}`
ğŸ¤– Ù…Ø¯Ù„: `{model}`
{token_info}{reranker_info}
---

"""
    return debug_header + answer

# Initialize memory services
memory_service: ConversationMemory = get_conversation_memory()
long_term_memory_service: LongTermMemoryService = get_long_term_memory_service()


# Request/Response Models (same as before)
class FileAttachment(BaseModel):
    """File attachment model with MinIO link."""
    filename: str = Field(..., description="Original filename")
    minio_url: str = Field(..., description="MinIO object key or full URL")
    file_type: str = Field(..., description="MIME type")
    size_bytes: Optional[int] = Field(None, description="File size in bytes")


class QueryRequest(BaseModel):
    """Query request model with file attachments."""
    query: str = Field(..., min_length=1, max_length=settings.max_query_length)
    conversation_id: Optional[str] = None
    language: str = Field(default="fa", pattern="^(fa|en|ar)$")
    max_results: int = Field(default=settings.rag_max_chunks, ge=1, le=20)
    filters: Optional[Dict[str, Any]] = None
    use_cache: bool = True
    use_reranking: bool = True
    user_preferences: Optional[Dict[str, Any]] = None
    file_attachments: Optional[List[FileAttachment]] = Field(None, max_items=5)
    enable_web_search: Optional[bool] = Field(
        default=None, 
        description="Enable web search for RAG responses. If None, uses server default (ENABLE_RAG_WEB_SEARCH). Set to True/False to override."
    )


class QueryResponse(BaseModel):
    """Query response model."""
    answer: str
    sources: list[str]
    conversation_id: str
    message_id: str
    tokens_used: int
    processing_time_ms: int
    file_analysis: Optional[str] = None  # ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
    context_used: bool = False  # Ø¢ÛŒØ§ Ø§Ø² Ø­Ø§ÙØ¸Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯


@router.post(
    "/",
    response_model=QueryResponse,
    summary="Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡",
    description="""
    Ø§ÛŒÙ† API Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ:
    
    **1. ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„ Ø¨Ø§ LLM:**
    - Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø¶Ù…ÛŒÙ…Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ LLM ØªØ­Ù„ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
    - Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² ØªØµÙˆÛŒØ±ØŒ PDFØŒ TXT
    - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² ÙØ§ÛŒÙ„
    
    **2. Ø­Ø§ÙØ¸Ù‡ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª:**
    - 10 Ù¾ÛŒØ§Ù… Ø¢Ø®Ø± Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
    - Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ù¾ÛŒÙˆØ³ØªÙ‡
    
    **3. Ø­Ø§ÙØ¸Ù‡ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª:**
    - Ø®Ù„Ø§ØµÙ‡ Ù…Ú©Ø§Ù„Ù…Ø§Øª Ù‚Ø¨Ù„ÛŒ Ú©Ø§Ø±Ø¨Ø±
    - Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø¹Ø¯ Ø§Ø² 20 Ù¾ÛŒØ§Ù…
    
    **4. Ú©Ù„Ø§Ø³ÛŒÙÛŒÚ©ÛŒØ´Ù† Ù‡ÙˆØ´Ù…Ù†Ø¯:**
    - Ø¨Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† context Ùˆ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
    - ØªØ´Ø®ÛŒØµ Ø³ÙˆØ§Ù„ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² Ú†Ø±Øªâ€ŒÙˆÙ¾Ø±Øª
    
    **5. ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®:**
    - Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙ…Ø§Ù… context
    - Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù…Ø±ØªØ¨Ø·
    """
)
async def process_query_enhanced(
    request: QueryRequest,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
    user_id: str = Depends(get_current_user_id)
) -> QueryResponse:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    
    start_time = datetime.utcnow()
    
    try:
        # ========== Ù…Ø±Ø­Ù„Ù‡ 1: Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª ==========
        # NOTE: Ú©Ù†ØªØ±Ù„ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø§Ø´ØªØ±Ø§Ú© Ø³Ù…Øª Ø³ÛŒØ³ØªÙ… Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
        user = await get_or_create_user(db, user_id)
        
        # ========== Ù…Ø±Ø­Ù„Ù‡ 2: Ù…Ø¯ÛŒØ±ÛŒØª Conversation ==========
        conversation = await get_or_create_conversation(
            db, user.id, request.conversation_id, request.query[:100]
        )
        
        # ========== Ù…Ø±Ø­Ù„Ù‡ 3: ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¶Ù…ÛŒÙ…Ù‡ (Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯) ==========
        file_analysis, files_content = await process_file_attachments(
            request.file_attachments,
            request.query,
            request.language
        )
        
        # ========== Ù…Ø±Ø­Ù„Ù‡ 4: Ø¯Ø±ÛŒØ§ÙØª Ø­Ø§ÙØ¸Ù‡ Ù…Ú©Ø§Ù„Ù…Ø§Øª ==========
        long_term_memory, short_term_memory, context_for_classification = await get_conversation_context(
            db, str(user.id), str(conversation.id)
        )
        
        # ========== Ù…Ø±Ø­Ù„Ù‡ 5: Ú©Ù„Ø§Ø³ÛŒÙÛŒÚ©ÛŒØ´Ù† Ø¯Ù‚ÛŒÙ‚ Ø³ÙˆØ§Ù„ ==========
        classification = None
        
        if settings.enable_query_classification:
            from app.llm.classifier import QueryClassifier
            classifier = QueryClassifier()
            
            classification = await classifier.classify(
                query=request.query,
                language=request.language,
                context=context_for_classification,
                file_analysis=file_analysis
            )
            
            logger.info(
                "Query classified",
                category=classification.category,
                confidence=classification.confidence,
                has_meaningful_files=classification.has_meaningful_files,
                needs_clarification=classification.needs_clarification
            )
            
            # ========== Ù‡Ù†Ø¯Ù„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù¾Ø§ÛŒÛŒÙ† ==========
            # Ø§Ú¯Ø± Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø²ÛŒØ± 50% Ø§Ø³ØªØŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªÙˆØ¶ÛŒØ­ Ú©Ù†
            # ØªÙˆØ¬Ù‡: needs_clarification Ø¨Ø±Ø§ÛŒ invalid Ù‡Ø§ Ø·Ø¨ÛŒØ¹ÛŒ Ø§Ø³Øª Ùˆ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù‡Ù†Ø¯Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
            if classification.confidence < 0.5 and classification.category not in ["invalid_no_file", "invalid_with_file"]:
                logger.info(
                    "Low confidence or needs clarification",
                    confidence=classification.confidence,
                    needs_clarification=classification.needs_clarification,
                    original_category=classification.category
                )
                
                # Ù¾Ø§Ø³Ø® Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªÙˆØ¶ÛŒØ­
                clarification_response = classification.direct_response or "Ù…ØªÙˆØ¬Ù‡ Ù…Ù†Ø¸ÙˆØ± Ø´Ù…Ø§ Ù†Ø´Ø¯Ù…. Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ ÛŒØ§ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø¶Ø­â€ŒØªØ± Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯."
                
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯ÛŒØ¨Ø§Ú¯
                clarification_response = add_debug_info(
                    answer=clarification_response,
                    category=f"{classification.category} (low_confidence)",
                    model="classifier",
                    input_tokens=0,
                    output_tokens=0,
                    confidence=classification.confidence
                )
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
                user_msg = DBMessage(
                    id=uuid.uuid4(),
                    conversation_id=conversation.id,
                    role=MessageRole.USER,
                    content=request.query,
                    created_at=datetime.utcnow()
                )
                db.add(user_msg)
                
                assistant_msg = DBMessage(
                    id=uuid.uuid4(),
                    conversation_id=conversation.id,
                    role=MessageRole.ASSISTANT,
                    content=clarification_response,
                    created_at=datetime.utcnow()
                )
                db.add(assistant_msg)
                
                conversation.message_count += 2
                user.increment_query_count()
                await db.commit()
                
                processing_time = int((datetime.utcnow() - start_time).total_seconds() * 1000)
                
                return QueryResponse(
                    answer=clarification_response,
                    sources=[],
                    conversation_id=str(conversation.id),
                    message_id=str(assistant_msg.id),
                    tokens_used=0,
                    processing_time_ms=processing_time,
                    file_analysis=file_analysis,
                    context_used=bool(short_term_memory or long_term_memory)
                )
            
            # ========== Ù…Ø³ÛŒØ± 1: invalid_no_file - Ù…ØªÙ† Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨Ø¯ÙˆÙ† ÙØ§ÛŒÙ„ ==========
            if classification.category == "invalid_no_file":
                logger.info("Handling invalid_no_file: asking for clarification")
                
                response_text = classification.direct_response or "Ù…ØªÙ† Ø´Ù…Ø§ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù… Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ÙˆØ§Ø¶Ø­ Ùˆ Ú©Ø§Ù…Ù„ Ø¨Ù¾Ø±Ø³ÛŒØ¯."
                
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯ÛŒØ¨Ø§Ú¯
                response_text = add_debug_info(
                    answer=response_text,
                    category=classification.category,
                    model="classifier",
                    input_tokens=0,
                    output_tokens=0,
                    confidence=classification.confidence
                )
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
                user_msg = DBMessage(
                    id=uuid.uuid4(),
                    conversation_id=conversation.id,
                    role=MessageRole.USER,
                    content=request.query,
                    created_at=datetime.utcnow()
                )
                db.add(user_msg)
                
                assistant_msg = DBMessage(
                    id=uuid.uuid4(),
                    conversation_id=conversation.id,
                    role=MessageRole.ASSISTANT,
                    content=response_text,
                    created_at=datetime.utcnow()
                )
                db.add(assistant_msg)
                
                conversation.message_count += 2
                user.increment_query_count()
                await db.commit()
                
                processing_time = int((datetime.utcnow() - start_time).total_seconds() * 1000)
                
                return QueryResponse(
                    answer=response_text,
                    sources=[],
                    conversation_id=str(conversation.id),
                    message_id=str(assistant_msg.id),
                    tokens_used=0,
                    processing_time_ms=processing_time,
                    file_analysis=None,
                    context_used=False
                )
            
            # ========== Ù…Ø³ÛŒØ± 2: invalid_with_file - Ù…ØªÙ† Ù…Ø¨Ù‡Ù… Ø¨Ø§ ÙØ§ÛŒÙ„ ==========
            elif classification.category == "invalid_with_file":
                logger.info(
                    "Handling invalid_with_file",
                    has_meaningful_files=classification.has_meaningful_files
                )
                
                # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø± Ø§Ø³ØªØŒ Ø³ÙˆØ§Ù„ Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡ Ø¨Ù¾Ø±Ø³
                # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø¨ÛŒâ€ŒÙ…Ø¹Ù†ÛŒ Ø§Ø³ØªØŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªÙˆØ¶ÛŒØ­ Ú©Ù†
                response_text = classification.direct_response or "Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø¶Ø­â€ŒØªØ± Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯."
                
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯ÛŒØ¨Ø§Ú¯
                response_text = add_debug_info(
                    answer=response_text,
                    category=classification.category,
                    model="classifier",
                    input_tokens=0,
                    output_tokens=0,
                    confidence=classification.confidence
                )
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
                user_msg = DBMessage(
                    id=uuid.uuid4(),
                    conversation_id=conversation.id,
                    role=MessageRole.USER,
                    content=f"{request.query}\n[ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¶Ù…ÛŒÙ…Ù‡: {', '.join([f.filename for f in request.file_attachments])}]" if request.file_attachments else request.query,
                    created_at=datetime.utcnow()
                )
                db.add(user_msg)
                
                assistant_msg = DBMessage(
                    id=uuid.uuid4(),
                    conversation_id=conversation.id,
                    role=MessageRole.ASSISTANT,
                    content=response_text,
                    created_at=datetime.utcnow()
                )
                db.add(assistant_msg)
                
                conversation.message_count += 2
                user.increment_query_count()
                await db.commit()
                
                processing_time = int((datetime.utcnow() - start_time).total_seconds() * 1000)
                
                return QueryResponse(
                    answer=response_text,
                    sources=[],
                    conversation_id=str(conversation.id),
                    message_id=str(assistant_msg.id),
                    tokens_used=0,
                    processing_time_ms=processing_time,
                    file_analysis=file_analysis,
                    context_used=False
                )
            
            # ========== Ù…Ø³ÛŒØ± 3: general - Ø³ÙˆØ§Ù„ Ø¹Ù…ÙˆÙ…ÛŒ ØºÛŒØ± Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø± ==========
            elif classification.category == "general":
                logger.info(
                    "Handling general: using LLM1 (Light) without RAG",
                    needs_web_search=classification.needs_web_search
                )
                
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LLM1 (Light) Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø³Ø§Ø¯Ù‡
                from app.llm.factory import get_llm_for_category
                from app.llm.base import Message
                from app.config.prompts import SystemPrompts
                
                llm = get_llm_for_category(classification.category)
                
                # Ø¯Ø±ÛŒØ§ÙØª ØªØ§Ø±ÛŒØ® Ùˆ Ø³Ø§Ø¹Øª ÙØ¹Ù„ÛŒ (Ø´Ù…Ø³ÛŒ)
                current_date_shamsi, current_time_fa = get_current_shamsi_datetime()
                
                system_message = SystemPrompts.get_system_identity(
                    current_date_shamsi=current_date_shamsi,
                    current_time_fa=current_time_fa
                )
                
                # Ø³Ø§Ø®Øª user message Ø¨Ø§ context
                user_message_parts = []
                
                # 1. Ø­Ø§ÙØ¸Ù‡ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª
                if long_term_memory:
                    user_message_parts.append(f"[Ø®Ù„Ø§ØµÙ‡ Ù…Ú©Ø§Ù„Ù…Ø§Øª Ù‚Ø¨Ù„ÛŒ]\n{long_term_memory}\n")
                
                # 2. Ø­Ø§ÙØ¸Ù‡ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª
                if short_term_memory:
                    memory_text = "\n".join([
                        f"{'Ú©Ø§Ø±Ø¨Ø±' if m['role'] == 'user' else 'Ø¯Ø³ØªÛŒØ§Ø±'}: {m['content']}"
                        for m in short_term_memory
                    ])
                    user_message_parts.append(f"[Ù…Ú©Ø§Ù„Ù…Ø§Øª Ø§Ø®ÛŒØ±]\n{memory_text}\n")
                
                # 3. ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„
                if file_analysis:
                    user_message_parts.append(f"[ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¶Ù…ÛŒÙ…Ù‡]\n{file_analysis}\n")
                
                # 4. Ø³ÙˆØ§Ù„ ÙØ¹Ù„ÛŒ
                user_message_parts.append(f"[Ø³ÙˆØ§Ù„ ÙØ¹Ù„ÛŒ]\n{request.query}")
                
                user_message = "\n".join(user_message_parts)
                
                messages = [
                    Message(role="system", content=system_message),
                    Message(role="user", content=user_message)
                ]
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØµØ§ÙˆÛŒØ± Ø§Ø² files_content
                images_for_llm = []
                if files_content:
                    for fc in files_content:
                        if fc.get('is_image') and fc.get('image_data'):
                            images_for_llm.append({
                                'data': fc['image_data'],
                                'filename': fc['filename']
                            })
                
                # Ø§Ù†ØªØ®Ø§Ø¨ Ø±ÙˆØ´ Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ
                if images_for_llm:
                    # Ø§Ú¯Ø± ØªØµÙˆÛŒØ± Ø¯Ø§Ø±ÛŒÙ…ØŒ Ø§Ø² Vision API Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
                    logger.info("Using Vision API for general query with images", image_count=len(images_for_llm))
                    llm_response = await llm.generate_with_images(messages, images_for_llm)
                    model_used = f"{settings.llm1_model} (vision)"
                elif classification.needs_web_search:
                    logger.info("Using web search for general query")
                    llm_response = await llm.generate_with_web_search(messages)
                    model_used = f"{settings.llm1_model} (web_search)"
                else:
                    llm_response = await llm.generate_responses_api(
                        messages,
                        reasoning_effort="low"
                    )
                    model_used = settings.llm1_model
                response_text = llm_response.content
                
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯ÛŒØ¨Ø§Ú¯
                response_text = add_debug_info(
                    answer=response_text,
                    category=classification.category,
                    model=model_used,
                    input_tokens=llm_response.usage.get("prompt_tokens", 0) if llm_response.usage else 0,
                    output_tokens=llm_response.usage.get("completion_tokens", 0) if llm_response.usage else 0,
                    confidence=classification.confidence
                )
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
                user_msg = DBMessage(
                    id=uuid.uuid4(),
                    conversation_id=conversation.id,
                    role=MessageRole.USER,
                    content=request.query,
                    created_at=datetime.utcnow()
                )
                db.add(user_msg)
                
                assistant_msg = DBMessage(
                    id=uuid.uuid4(),
                    conversation_id=conversation.id,
                    role=MessageRole.ASSISTANT,
                    content=response_text,
                    created_at=datetime.utcnow()
                )
                db.add(assistant_msg)
                
                conversation.message_count += 2
                user.increment_query_count()
                
                # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±
                input_tokens = llm_response.usage.get("prompt_tokens", 0) if llm_response.usage else 0
                output_tokens = llm_response.usage.get("completion_tokens", 0) if llm_response.usage else 0
                total_tokens = llm_response.usage.get("total_tokens", 0) if llm_response.usage else 0
                user.total_tokens_used += total_tokens
                user.total_input_tokens += input_tokens
                user.total_output_tokens += output_tokens
                
                await db.commit()
                
                processing_time = int((datetime.utcnow() - start_time).total_seconds() * 1000)
                
                return QueryResponse(
                    answer=response_text,
                    sources=[],
                    conversation_id=str(conversation.id),
                    message_id=str(assistant_msg.id),
                    tokens_used=total_tokens,
                    processing_time_ms=processing_time,
                    file_analysis=file_analysis,
                    context_used=False
                )
            
            # ========== Ù…Ø³ÛŒØ± 4 Ùˆ 5: business_no_file Ùˆ business_with_file ==========
            # Ø§ÛŒÙ† Ø¯Ùˆ Ù…Ø³ÛŒØ± Ø¨Ù‡ RAG Pipeline Ù…ÛŒâ€ŒØ±ÙˆÙ†Ø¯ (Ø§Ø¯Ø§Ù…Ù‡ Ú©Ø¯ ÙØ¹Ù„ÛŒ)
        
        # ========== Ù…Ø±Ø­Ù„Ù‡ 6: Ø³Ø§Ø®Øª Query Ùˆ Context Ø¨Ø±Ø§ÛŒ RAG ==========
        # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² hallucination:
        # - Query Ø¨Ø±Ø§ÛŒ embedding: ÙÙ‚Ø· Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ (Ø¨Ø¯ÙˆÙ† context)
        # - Context Ø¨Ø±Ø§ÛŒ LLM: Ø´Ø§Ù…Ù„ Ø­Ø§ÙØ¸Ù‡ + ÙØ§ÛŒÙ„ + Ø³ÙˆØ§Ù„
        
        # Query Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ (ÙÙ‚Ø· Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ)
        search_query = request.query
        
        # Context Ø¨Ø±Ø§ÛŒ LLM (Ø´Ø§Ù…Ù„ Ù‡Ù…Ù‡ Ú†ÛŒØ²) - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² utility Ù…Ø´ØªØ±Ú©
        llm_context = build_llm_context(
            request.query,
            long_term_memory,
            short_term_memory,
            file_analysis
        )
        
        logger.info(
            "RAG query prepared",
            search_query_length=len(search_query),
            llm_context_length=len(llm_context),
            has_file_analysis=bool(file_analysis),
            has_memory=bool(long_term_memory or short_term_memory)
        )
        
        # ========== Ù…Ø±Ø­Ù„Ù‡ 7: Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ RAG Pipeline ==========
        # ØªØ¹ÛŒÛŒÙ† ÙˆØ¶Ø¹ÛŒØª web search:
        # 1. Classifier ØªØµÙ…ÛŒÙ… Ù†Ù‡Ø§ÛŒÛŒ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ (needs_web_search)
        # 2. Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± enable_web_search=false ÙØ±Ø³ØªØ§Ø¯Ù‡ØŒ Ø­ØªÛŒ Ø§Ú¯Ø± classifier Ø¨Ú¯ÙˆÛŒØ¯ trueØŒ ØºÛŒØ±ÙØ¹Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        # 3. Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± enable_web_search=true ÙØ±Ø³ØªØ§Ø¯Ù‡ ÙˆÙ„ÛŒ classifier Ø¨Ú¯ÙˆÛŒØ¯ falseØŒ ØºÛŒØ±ÙØ¹Ø§Ù„ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯ (classifier Ø§ÙˆÙ„ÙˆÛŒØª Ø¯Ø§Ø±Ø¯)
        # 4. Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± Ú†ÛŒØ²ÛŒ Ù†ÙØ±Ø³ØªØ§Ø¯Ù‡ØŒ ÙÙ‚Ø· ØªØµÙ…ÛŒÙ… classifier Ù…Ù„Ø§Ú© Ø§Ø³Øª
        
        # ØªØµÙ…ÛŒÙ… classifier
        classifier_wants_web_search = classification.needs_web_search
        
        # ØªØµÙ…ÛŒÙ… Ù†Ù‡Ø§ÛŒÛŒ: classifier Ø¨Ø§ÛŒØ¯ Ø¨Ú¯ÙˆÛŒØ¯ Ù†ÛŒØ§Ø² Ø§Ø³Øª AND Ú©Ø§Ø±Ø¨Ø± Ù†Ø¨Ø§ÛŒØ¯ ØµØ±ÛŒØ­Ø§Ù‹ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
        # Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø¢ÛŒØ§ Ú©Ø§Ø±Ø¨Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨ Ø±Ø§ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù‡ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨ÙˆØ¯
        web_search_blocked_by_user = False
        
        if request.enable_web_search is False:
            # Ú©Ø§Ø±Ø¨Ø± ØµØ±ÛŒØ­Ø§Ù‹ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù‡
            web_search_enabled = False
            if classifier_wants_web_search:
                # classifier Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯ Ù†ÛŒØ§Ø² Ø§Ø³Øª ÙˆÙ„ÛŒ Ú©Ø§Ø±Ø¨Ø± ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù‡
                web_search_blocked_by_user = True
        else:
            # classifier ØªØµÙ…ÛŒÙ… Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯
            web_search_enabled = classifier_wants_web_search
        
        logger.info(
            "Web search decision",
            classifier_decision=classifier_wants_web_search,
            user_preference=request.enable_web_search,
            final_decision=web_search_enabled
        )
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØµØ§ÙˆÛŒØ± Ø§Ø² files_content Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ LLM
        images_for_rag = []
        if files_content:
            for fc in files_content:
                if fc.get('is_image') and fc.get('image_data'):
                    images_for_rag.append({
                        'data': fc['image_data'],
                        'filename': fc['filename']
                    })
        
        rag_query = RAGQuery(
            text=search_query,  # ÙÙ‚Ø· Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ embedding
            user_id=str(user.id),
            conversation_id=str(conversation.id),
            language=request.language,
            max_chunks=request.max_results,
            filters=request.filters,
            use_cache=request.use_cache,
            use_reranking=request.use_reranking,
            user_preferences=request.user_preferences,
            enable_web_search=web_search_enabled,  # Web search Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØµÙ…ÛŒÙ… classifier Ùˆ ØªØ±Ø¬ÛŒØ­ Ú©Ø§Ø±Ø¨Ø±
            # ÙÛŒÙ„ØªØ± Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ´Ø®ÛŒØµ classifier
            temporal_context=classification.temporal_context if classification else None,
            target_date=classification.target_date if classification else None,
            # ØªØµØ§ÙˆÛŒØ± Ø¨Ø±Ø§ÛŒ Vision API
            images=images_for_rag if images_for_rag else None
        )
        
        pipeline = RAGPipeline()
        rag_response = await pipeline.process(
            rag_query,
            additional_context=llm_context,  # Context Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ LLM
            skip_classification=True  # Classification Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡
        )
        
        # ========== Ù…Ø±Ø­Ù„Ù‡ 8: Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ ==========
        # Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±
        user_message_content = request.query
        if request.file_attachments:
            file_info = "\n[ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¶Ù…ÛŒÙ…Ù‡: " + ", ".join(
                [f.filename for f in request.file_attachments]
            ) + "]"
            user_message_content += file_info
        
        user_message = DBMessage(
            id=uuid.uuid4(),
            conversation_id=conversation.id,
            role=MessageRole.USER,
            content=user_message_content,
            created_at=datetime.utcnow()
        )
        db.add(user_message)
        
        # Ù¾ÛŒØ§Ù… Ø¯Ø³ØªÛŒØ§Ø±
        assistant_message = DBMessage(
            id=uuid.uuid4(),
            conversation_id=conversation.id,
            role=MessageRole.ASSISTANT,
            content=rag_response.answer,
            tokens=rag_response.total_tokens,
            processing_time_ms=rag_response.processing_time_ms,
            retrieved_chunks=[
                {
                    "text": chunk.text,
                    "score": chunk.score,
                    "source": chunk.source,
                    "metadata": chunk.metadata
                }
                for chunk in rag_response.chunks
            ],
            sources=rag_response.sources,
            model_used=rag_response.model_used,
            created_at=datetime.utcnow()
        )
        db.add(assistant_message)
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ conversation
        conversation.message_count += 2
        conversation.total_tokens += rag_response.total_tokens
        conversation.last_message_at = datetime.utcnow()
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ user
        user.increment_query_count()
        user.total_tokens_used += rag_response.total_tokens
        user.total_input_tokens += rag_response.input_tokens
        user.total_output_tokens += rag_response.output_tokens
        
        await db.commit()
        
        # ========== Ù…Ø±Ø­Ù„Ù‡ 9: Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø­Ø§ÙØ¸Ù‡â€ŒÙ‡Ø§ (Background) ==========
        # 9.1: Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø­Ø§ÙØ¸Ù‡ Ú†Øª (Ø®Ù„Ø§ØµÙ‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ)
        background_tasks.add_task(
            memory_service.update_long_term_memory,
            db,
            str(conversation.id),
            force=False
        )
        
        # 9.2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ø§ÙØ¸Ù‡ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ú©Ø§Ø±Ø¨Ø± (Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒØ¯Ø§Ø±)
        background_tasks.add_task(
            _extract_and_save_user_memory,
            db,
            str(user.id),
            str(conversation.id),
            request.query,
            rag_response.answer,
            context_for_classification
        )
        
        # ========== Ù…Ø±Ø­Ù„Ù‡ 10: Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù¾Ø§Ø³Ø® ==========
        processing_time = int((datetime.utcnow() - start_time).total_seconds() * 1000)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯ÛŒØ¨Ø§Ú¯ Ø¨Ù‡ Ù¾Ø§Ø³Ø® RAG
        model_display = rag_response.model_used or settings.llm2_model
        if web_search_enabled:
            model_display = f"{model_display} (web_search)"
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ§Ù… Ù‡Ø´Ø¯Ø§Ø± Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨ Ø±Ø§ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù‡ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨ÙˆØ¯
        answer_with_warning = rag_response.answer
        if web_search_blocked_by_user:
            web_search_warning = "\n\n---\nâš ï¸ **ØªÙˆØ¬Ù‡:** Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¨Ù‡ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§ÛŒÙ†ØªØ±Ù†Øª Ø¨ÙˆØ¯ Ú©Ù‡ Ø¯Ø± ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø´Ù…Ø§ ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ù‡â€ŒØ±ÙˆØ²ØªØ±ØŒ Ù„Ø·ÙØ§Ù‹ Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨ Ø±Ø§ Ø¯Ø± ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯."
            answer_with_warning = rag_response.answer + web_search_warning
        
        final_answer = add_debug_info(
            answer=answer_with_warning,
            category=classification.category,
            model=model_display,
            input_tokens=rag_response.input_tokens,
            output_tokens=rag_response.output_tokens,
            confidence=classification.confidence,
            cached=rag_response.cached,
            reranker_details=rag_response.reranker_details
        )
        
        return QueryResponse(
            answer=final_answer,
            sources=rag_response.sources,
            conversation_id=str(conversation.id),
            message_id=str(assistant_message.id),
            tokens_used=rag_response.total_tokens,
            processing_time_ms=processing_time,
            file_analysis=file_analysis,
            context_used=bool(long_term_memory or short_term_memory)
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Query processing failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to process query: {str(e)}"
        )


async def _extract_and_save_user_memory(
    db: AsyncSession,
    user_id: str,
    conversation_id: str,
    user_message: str,
    assistant_response: str,
    conversation_context: Optional[str]
):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø­Ø§ÙØ¸Ù‡ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ú©Ø§Ø±Ø¨Ø± (Background Task)
    
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø¹Ø¯ Ø§Ø² Ù‡Ø± Ù¾Ø§Ø³Ø® Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§
    Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ ÛŒØ§ Ø®ÛŒØ±.
    """
    try:
        # Ù…Ø±Ø­Ù„Ù‡ 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ø§ÙØ¸Ù‡ Ø§Ø² Ù¾ÛŒØ§Ù…
        extraction = await long_term_memory_service.extract_memory_from_message(
            user_message=user_message,
            assistant_response=assistant_response,
            conversation_context=conversation_context
        )
        
        # Ù…Ø±Ø­Ù„Ù‡ 2: Ø§Ú¯Ø± Ø­Ø§ÙØ¸Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
        if extraction.get("should_write_memory") and extraction.get("memory_to_write"):
            # Ù…Ø±Ø­Ù„Ù‡ 3: Ø§Ø¯ØºØ§Ù… Ø¨Ø§ Ø­Ø§ÙØ¸Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
            result = await long_term_memory_service.merge_memory(
                db=db,
                user_id=user_id,
                new_memory=extraction["memory_to_write"],
                category=extraction.get("category", "other"),
                conversation_id=conversation_id
            )
            
            logger.info(
                "User memory extraction completed",
                user_id=user_id,
                action=result.get("action"),
                memory_content=extraction["memory_to_write"][:50]
            )
        else:
            logger.debug(
                "No memory to extract from message",
                user_id=user_id
            )
            
    except Exception as e:
        # Background task - ÙÙ‚Ø· Ù„Ø§Ú¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ Ø®Ø·Ø§ Ù†Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
        logger.error(f"Failed to extract user memory: {e}")
