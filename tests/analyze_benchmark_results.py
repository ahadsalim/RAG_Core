#!/usr/bin/env python3
"""ØªØ­Ù„ÛŒÙ„ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†ØªØ§ÛŒØ¬ benchmark Ù‡Ø§ÛŒ LLM"""
import json
import glob
from typing import List, Dict, Any
from pathlib import Path

def load_all_results() -> List[Dict[str, Any]]:
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù†ØªØ§ÛŒØ¬"""
    results = []
    pattern = '/tmp/llm_test_*.json'
    
    for filepath in glob.glob(pattern):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                results.append(data)
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† {filepath}: {e}")
    
    return results

def analyze_results(all_results: List[Dict[str, Any]]):
    """ØªØ­Ù„ÛŒÙ„ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†ØªØ§ÛŒØ¬"""
    
    print("="*80)
    print("ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ù†ØªØ§ÛŒØ¬ Benchmark")
    print("="*80)
    print(f"\nØªØ¹Ø¯Ø§Ø¯ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØªØ³Øª Ø´Ø¯Ù‡: {len(all_results)}")
    
    # Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡
    print("\n" + "="*80)
    print("Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Provider Ùˆ Model")
    print("="*80)
    print(f"{'Provider':<10} {'Model':<25} {'Ù…ÙˆÙÙ‚':<8} {'Ú©Ù„(ms)':<10} {'Ù¾Ø±Ø¯Ø§Ø²Ø´(ms)':<12} {'ØªÙˆÚ©Ù†':<8}")
    print("-"*80)
    
    summary_data = []
    
    for data in all_results:
        config = data['config']
        results = data['results']
        
        successful = [r for r in results if r['success']]
        if not successful:
            continue
        
        provider = config['provider']
        model = config['model']
        count = len(successful)
        total_queries = len(results)
        
        avg_total = sum(r['total_time_ms'] for r in successful) / count
        avg_proc = sum(r['processing_time_ms'] for r in successful) / count
        avg_tokens = sum(r['tokens_used'] for r in successful) / count
        
        summary_data.append({
            'provider': provider,
            'model': model,
            'count': count,
            'total': total_queries,
            'avg_total_ms': avg_total,
            'avg_proc_ms': avg_proc,
            'avg_tokens': avg_tokens
        })
        
        print(f"{provider:<10} {model:<25} {count}/{total_queries:<6} {avg_total:<10.0f} {avg_proc:<12.0f} {avg_tokens:<8.0f}")
    
    # Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§
    if summary_data:
        print("\n" + "="*80)
        print("ğŸ† Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§")
        print("="*80)
        
        fastest = min(summary_data, key=lambda x: x['avg_total_ms'])
        print(f"\nâš¡ Ø³Ø±ÛŒØ¹â€ŒØªØ±ÛŒÙ†: {fastest['provider']} - {fastest['model']}")
        print(f"   Ø²Ù…Ø§Ù†: {fastest['avg_total_ms']:.0f}ms")
        
        least_tokens = min(summary_data, key=lambda x: x['avg_tokens'])
        print(f"\nğŸ’° Ú©Ù…â€ŒØªØ±ÛŒÙ† ØªÙˆÚ©Ù†: {least_tokens['provider']} - {least_tokens['model']}")
        print(f"   ØªÙˆÚ©Ù†: {least_tokens['avg_tokens']:.0f}")
        
        # ØªÙˆØµÛŒÙ‡
        print("\n" + "="*80)
        print("ğŸ’¡ ØªÙˆØµÛŒÙ‡")
        print("="*80)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² (Ø³Ø±Ø¹Øª + Ú©Ø§Ø±Ø§ÛŒÛŒ ØªÙˆÚ©Ù†)
        for item in summary_data:
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ (Ú©Ù…ØªØ± Ø¨Ù‡ØªØ±)
            speed_score = item['avg_total_ms'] / 1000  # Ø¨Ù‡ Ø«Ø§Ù†ÛŒÙ‡
            token_score = item['avg_tokens'] / 1000  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
            item['combined_score'] = speed_score + token_score
        
        best_overall = min(summary_data, key=lambda x: x['combined_score'])
        print(f"\nğŸ¯ Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù„ÛŒ: {best_overall['provider']} - {best_overall['model']}")
        print(f"   Ø²Ù…Ø§Ù†: {best_overall['avg_total_ms']:.0f}ms")
        print(f"   ØªÙˆÚ©Ù†: {best_overall['avg_tokens']:.0f}")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
    report_file = '/tmp/benchmark_comparison_report.json'
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump({
            'summary': summary_data,
            'fastest': fastest if summary_data else None,
            'least_tokens': least_tokens if summary_data else None,
            'best_overall': best_overall if summary_data else None
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø¯Ø± {report_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

def main():
    results = load_all_results()
    
    if not results:
        print("âŒ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
        print("Ø§Ø¨ØªØ¯Ø§ ØªØ³Øªâ€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯:")
        print("  bash /app/tests/run_comprehensive_benchmark.sh")
        return 1
    
    analyze_results(results)
    return 0

if __name__ == '__main__':
    exit(main())
