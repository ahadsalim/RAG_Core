# Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Local Embedding

## ğŸ¯ Ù…Ø¯Ù„ Ù†ØµØ¨ Ø´Ø¯Ù‡

**Ù…Ø¯Ù„**: `intfloat/multilingual-e5-base`
- **Ø¨ÙØ¹Ø¯**: 768
- **Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§**: ÙØ§Ø±Ø³ÛŒØŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒØŒ Ø¹Ø±Ø¨ÛŒØŒ Ùˆ 100+ Ø²Ø¨Ø§Ù† Ø¯ÛŒÚ¯Ø±
- **Ø­Ø¬Ù…**: 1.1 GB
- **Ø¯Ø³ØªÚ¯Ø§Ù‡**: CPU (ÛŒØ§ GPU Ø§Ú¯Ø± CUDA Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)

---

## ğŸš€ API Endpoints

### 1ï¸âƒ£ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„

```bash
curl http://localhost:7001/api/v1/embeddings/info
```

**Ø®Ø±ÙˆØ¬ÛŒ**:
```json
{
    "model": "intfloat/multilingual-e5-base",
    "dimension": 768,
    "device": "cpu",
    "status": "ready"
}
```

---

### 2ï¸âƒ£ Ø³Ø§Ø®Øª Embedding (ÛŒÚ© Ù…ØªÙ†)

```bash
curl -X POST http://localhost:7001/api/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "input": "Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§Ø± Ø§ÛŒØ±Ø§Ù†"
  }'
```

**Ø®Ø±ÙˆØ¬ÛŒ**:
```json
{
    "object": "list",
    "data": [
        {
            "object": "embedding",
            "embedding": [0.034, -0.021, ...],  // 768 Ø¹Ø¯Ø¯
            "index": 0
        }
    ],
    "model": "intfloat/multilingual-e5-base",
    "usage": {
        "prompt_tokens": 5,
        "total_tokens": 5
    }
}
```

---

### 3ï¸âƒ£ Ø³Ø§Ø®Øª Embedding (Ú†Ù†Ø¯ Ù…ØªÙ†)

```bash
curl -X POST http://localhost:7001/api/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "input": [
        "Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§Ø± Ø§ÛŒØ±Ø§Ù†",
        "Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…ÛŒ",
        "Ø­Ù‚ÙˆÙ‚ Ú©Ø§Ø±Ú¯Ø±Ø§Ù†"
    ]
  }'
```

---

### 4ï¸âƒ£ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª (Similarity)

```bash
curl -X POST http://localhost:7001/api/v1/embeddings/similarity \
  -H "Content-Type: application/json" \
  -d '{
    "text1": "Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§Ø± Ø§ÛŒØ±Ø§Ù†",
    "text2": "Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…ÛŒ Ø¯Ø± Ø§ÛŒØ±Ø§Ù†"
  }'
```

**Ø®Ø±ÙˆØ¬ÛŒ**:
```json
{
    "text1": "Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§Ø± Ø§ÛŒØ±Ø§Ù†",
    "text2": "Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…ÛŒ Ø¯Ø± Ø§ÛŒØ±Ø§Ù†",
    "similarity": 0.9293810725212097,
    "model": "intfloat/multilingual-e5-base"
}
```

**Ù†Ú©ØªÙ‡**: Similarity Ø¨ÛŒÙ† -1 ØªØ§ 1 Ø§Ø³Øª:
- `1.0` = Ú©Ø§Ù…Ù„Ø§Ù‹ Ù…Ø´Ø§Ø¨Ù‡
- `0.0` = Ø¨ÛŒâ€ŒØ±Ø¨Ø·
- `-1.0` = Ú©Ø§Ù…Ù„Ø§Ù‹ Ù…Ø®Ø§Ù„Ù

---

## ğŸ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Python

### Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡:
```bash
pip install requests
```

### Ø³Ø§Ø®Øª Embedding:
```python
import requests
import numpy as np

def get_embedding(text: str) -> np.ndarray:
    """Ø¯Ø±ÛŒØ§ÙØª embedding Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù…ØªÙ†."""
    response = requests.post(
        "http://localhost:7001/api/v1/embeddings",
        json={"input": text}
    )
    
    data = response.json()
    embedding = np.array(data["data"][0]["embedding"])
    
    return embedding


# Ù…Ø«Ø§Ù„
embedding = get_embedding("Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§Ø± Ø§ÛŒØ±Ø§Ù†")
print(f"Dimension: {len(embedding)}")
print(f"First 5 values: {embedding[:5]}")
```

### Ù…Ø­Ø§Ø³Ø¨Ù‡ Similarity:
```python
def calculate_similarity(text1: str, text2: str) -> float:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Ø¯Ùˆ Ù…ØªÙ†."""
    response = requests.post(
        "http://localhost:7001/api/v1/embeddings/similarity",
        json={"text1": text1, "text2": text2}
    )
    
    data = response.json()
    return data["similarity"]


# Ù…Ø«Ø§Ù„
similarity = calculate_similarity(
    "Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§Ø± Ø§ÛŒØ±Ø§Ù†",
    "Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…ÛŒ"
)
print(f"Similarity: {similarity:.4f}")
```

### Embedding Ú†Ù†Ø¯ÛŒÙ† Ù…ØªÙ†:
```python
def get_batch_embeddings(texts: list[str]) -> np.ndarray:
    """Ø¯Ø±ÛŒØ§ÙØª embedding Ø¨Ø±Ø§ÛŒ Ú†Ù†Ø¯ÛŒÙ† Ù…ØªÙ†."""
    response = requests.post(
        "http://localhost:7001/api/v1/embeddings",
        json={"input": texts}
    )
    
    data = response.json()
    embeddings = [np.array(item["embedding"]) for item in data["data"]]
    
    return np.array(embeddings)


# Ù…Ø«Ø§Ù„
texts = ["Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§Ø±", "Ù‚ÙˆØ§Ù†ÛŒÙ† Ù…Ø§Ù„ÛŒØ§ØªÛŒ", "Ø­Ù‚ÙˆÙ‚ Ú©Ø§Ø±Ú¯Ø±Ø§Ù†"]
embeddings = get_batch_embeddings(texts)
print(f"Shape: {embeddings.shape}")  # (3, 768)
```

---

## ğŸ”§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¯Ø± Ú©Ø¯

Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø§Ø² Ø³Ø±ÙˆÛŒØ³ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:

```python
from app.services.local_embedding_service import get_local_embedding_service

# Ø¯Ø±ÛŒØ§ÙØª Ø³Ø±ÙˆÛŒØ³
embedding_service = get_local_embedding_service()

# ÛŒÚ© Ù…ØªÙ†
embedding = embedding_service.encode_single("Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§Ø± Ø§ÛŒØ±Ø§Ù†")
print(f"Shape: {embedding.shape}")  # (768,)

# Ú†Ù†Ø¯ Ù…ØªÙ†
texts = ["Ù…ØªÙ† Û±", "Ù…ØªÙ† Û²", "Ù…ØªÙ† Û³"]
embeddings = embedding_service.encode(texts)
print(f"Shape: {embeddings.shape}")  # (3, 768)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª
similarity = embedding_service.similarity(embeddings[0], embeddings[1])
print(f"Similarity: {similarity}")
```

---

## ğŸŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù¾Ø±ÙˆÚ˜Ù‡ Ingest

Ø¯Ø± Ù¾Ø±ÙˆÚ˜Ù‡ IngestØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø§ÛŒÙ† endpoint Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:

### ØªÙ†Ø¸ÛŒÙ… .env Ø¯Ø± Ingest:
```bash
# Ø¯Ø± /home/ahad/project/ingest/.env
EMBEDDING_BASE_URL="http://localhost:7001/api/v1"
EMBEDDING_MODEL="intfloat/multilingual-e5-base"
```

### Ú©Ø¯ Python Ø¯Ø± Ingest:
```python
import requests

def get_embeddings_from_core(texts: list[str]):
    """Ø¯Ø±ÛŒØ§ÙØª embeddings Ø§Ø² Core API."""
    response = requests.post(
        "http://localhost:7001/api/v1/embeddings",
        json={"input": texts}
    )
    
    data = response.json()
    embeddings = [item["embedding"] for item in data["data"]]
    
    return embeddings
```

---

## ğŸ“Š Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ OpenAI API

Ø§ÛŒÙ† endpoint Ú©Ø§Ù…Ù„Ø§Ù‹ Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ OpenAI Embedding API Ø§Ø³Øª:

```python
# Ø¨Ù‡ Ø¬Ø§ÛŒ OpenAI:
from openai import OpenAI
client = OpenAI(api_key="...")
response = client.embeddings.create(
    input="Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§Ø±",
    model="text-embedding-3-large"
)

# Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Core Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:
from openai import OpenAI
client = OpenAI(
    api_key="not-needed",
    base_url="http://localhost:7001/api/v1"
)
response = client.embeddings.create(
    input="Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§Ø±",
    model="intfloat/multilingual-e5-base"
)
```

---

## ğŸš€ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯

### 1. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² GPU (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª)

Ù…Ø¯Ù„ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± GPU Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ:

```bash
curl http://localhost:7001/api/v1/embeddings/info
```

Ø§Ú¯Ø± `"device": "cuda"` Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø² GPU Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

### 2. Batch Processing

Ø¨Ø±Ø§ÛŒ Ú†Ù†Ø¯ Ù…ØªÙ†ØŒ Ø­ØªÙ…Ø§Ù‹ batch Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø§Ø³Øª):

```python
# âŒ Ú©Ù†Ø¯
embeddings = [get_embedding(text) for text in texts]

# âœ… Ø³Ø±ÛŒØ¹
embeddings = get_batch_embeddings(texts)
```

### 3. Normalization

Embedding Ù‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ normalize Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ (Ø¨Ø±Ø§ÛŒ cosine similarity Ø¨Ù‡ØªØ± Ø§Ø³Øª).

---

## ğŸ¯ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ

### 1. Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ (Semantic Search)

```python
import numpy as np
from typing import List, Tuple

def semantic_search(
    query: str,
    documents: List[str],
    top_k: int = 5
) -> List[Tuple[int, float]]:
    """Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯."""
    
    # Embedding query Ùˆ documents
    all_texts = [query] + documents
    embeddings = get_batch_embeddings(all_texts)
    
    query_emb = embeddings[0]
    doc_embs = embeddings[1:]
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª
    similarities = np.dot(doc_embs, query_emb)
    
    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ
    top_indices = np.argsort(similarities)[::-1][:top_k]
    
    results = [(idx, similarities[idx]) for idx in top_indices]
    return results


# Ù…Ø«Ø§Ù„
documents = [
    "Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§Ø± Ø§ÛŒØ±Ø§Ù† Ø¯Ø± Ø³Ø§Ù„ 1369 ØªØµÙˆÛŒØ¨ Ø´Ø¯",
    "Ù‚ÙˆØ§Ù†ÛŒÙ† Ù…Ø§Ù„ÛŒØ§ØªÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…",
    "Ø­Ù‚ÙˆÙ‚ Ùˆ Ø¯Ø³ØªÙ…Ø²Ø¯ Ú©Ø§Ø±Ú¯Ø±Ø§Ù†",
    "Ù‚Ø§Ù†ÙˆÙ† ØªØ¬Ø§Ø±Øª Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©"
]

results = semantic_search("Ù‚ÙˆØ§Ù†ÛŒÙ† Ú©Ø§Ø±Ú¯Ø±ÛŒ", documents, top_k=2)

for idx, score in results:
    print(f"{score:.3f}: {documents[idx]}")
```

### 2. Clustering (Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ)

```python
from sklearn.cluster import KMeans

def cluster_documents(documents: List[str], n_clusters: int = 3):
    """Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³Ù†Ø§Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØªÙˆØ§."""
    
    # Ø¯Ø±ÛŒØ§ÙØª embeddings
    embeddings = get_batch_embeddings(documents)
    
    # Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(embeddings)
    
    # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
    clusters = {}
    for idx, label in enumerate(labels):
        if label not in clusters:
            clusters[label] = []
        clusters[label].append(documents[idx])
    
    return clusters


# Ù…Ø«Ø§Ù„
docs = [
    "Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§Ø±",
    "Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…ÛŒ",
    "Ù‚ÙˆØ§Ù†ÛŒÙ† Ù…Ø§Ù„ÛŒØ§ØªÛŒ",
    "Ù…Ø§Ù„ÛŒØ§Øª Ø¨Ø± Ø¯Ø±Ø¢Ù…Ø¯",
    "Ø­Ù‚ÙˆÙ‚ Ú©Ø§Ø±Ú¯Ø±Ø§Ù†",
]

clusters = cluster_documents(docs, n_clusters=2)
for cluster_id, texts in clusters.items():
    print(f"\nCluster {cluster_id}:")
    for text in texts:
        print(f"  - {text}")
```

---

## ğŸ” Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ OpenAI

| ÙˆÛŒÚ˜Ú¯ÛŒ | Local (multilingual-e5-base) | OpenAI (text-embedding-3-large) |
|-------|------------------------------|----------------------------------|
| Ø¨ÙØ¹Ø¯ | 768 | 3072 |
| Ù‡Ø²ÛŒÙ†Ù‡ | Ø±Ø§ÛŒÚ¯Ø§Ù† | $0.13 / 1M tokens |
| Ø³Ø±Ø¹Øª | Ù…ØªÙˆØ³Ø· (CPU) / Ø³Ø±ÛŒØ¹ (GPU) | Ø³Ø±ÛŒØ¹ |
| Ø¢ÙÙ„Ø§ÛŒÙ† | âœ… Ø¨Ù„Ù‡ | âŒ Ø®ÛŒØ± |
| ÙØ§Ø±Ø³ÛŒ | âœ… Ø¹Ø§Ù„ÛŒ | âœ… Ø¹Ø§Ù„ÛŒ |
| Ø­Ø±ÛŒÙ… Ø®ØµÙˆØµÛŒ | âœ… Ú©Ø§Ù…Ù„ | âŒ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Ø³Ø±ÙˆØ± |

---

## ğŸ›  Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ

### Ø®Ø·Ø§: "Connection refused"
```bash
# Ú†Ú© Ú©Ù†ÛŒØ¯ API Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§Ø³Øª
curl http://localhost:7001/health
```

### Ø®Ø·Ø§: "Model not loaded"
```bash
# Restart API
pkill -f uvicorn
cd /home/ahad/project/core
source venv/bin/activate
uvicorn app.main:app --host 0.0.0.0 --port 7001 --reload
```

### Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ù†Ø¯
- Ø§Ø² batch processing Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
- Ø§Ú¯Ø± GPU Ø¯Ø§Ø±ÛŒØ¯ØŒ PyTorch Ø¨Ø§ CUDA Ù†ØµØ¨ Ú©Ù†ÛŒØ¯
- Ù…Ø¯Ù„ Ú©ÙˆÚ†Ú©â€ŒØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (e5-small)

---

## ğŸ“š Ù…Ù†Ø§Ø¨Ø¹

- **Ù…Ø¯Ù„**: https://huggingface.co/intfloat/multilingual-e5-base
- **Ù…Ù‚Ø§Ù„Ù‡**: https://arxiv.org/abs/2402.05672
- **Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡**: https://www.sbert.net

---

**ğŸ‰ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø³Øª!**
