#!/usr/bin/env python3
"""
ØªØ³Øª Ø¬Ø§Ù…Ø¹ 20 Ø³ÙˆØ§Ù„ ØªØ¬Ø§Ø±ÛŒ Ø¨Ø§ ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ù‡Ø± Ù…Ø±Ø­Ù„Ù‡
Ù‡Ø¯Ù: Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú¯Ù„ÙˆÚ¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ OpenAI vs GapGPT
"""
import sys
sys.path.insert(0, '/app')

import httpx
import time
import json
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any
from app.config.settings import settings

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
JWT_SECRET = settings.jwt_secret_key
CORE_API_URL = "http://localhost:7001"

# 20 Ø³ÙˆØ§Ù„ ØªØ®ØµØµÛŒ Ù…ØªÙ†ÙˆØ¹
BUSINESS_QUERIES = [
    # Ù…Ø§Ù„ÛŒØ§Øª (7 Ø³ÙˆØ§Ù„)
    'Ù…Ø§Ù„ÛŒØ§Øª Ø¨Ø± Ø§Ø±Ø²Ø´ Ø§ÙØ²ÙˆØ¯Ù‡ Ú†ÛŒØ³Øª Ùˆ Ú†Ú¯ÙˆÙ†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ',
    'Ù†Ø±Ø® Ù…Ø§Ù„ÛŒØ§Øª Ø¨Ø± Ø¯Ø±Ø¢Ù…Ø¯ Ø§Ø´Ø®Ø§Øµ Ø­Ù‚ÛŒÙ‚ÛŒ Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ',
    'Ù…Ø¹Ø§ÙÛŒØª Ù…Ø§Ù„ÛŒØ§ØªÛŒ Ø­Ù‚ÙˆÙ‚ Ú†Ú¯ÙˆÙ†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ',
    'Ù…Ø§Ù„ÛŒØ§Øª Ø¨Ø± Ø§Ø¬Ø§Ø±Ù‡ Ø§Ù…Ù„Ø§Ú© Ú†Ú¯ÙˆÙ†Ù‡ Ù¾Ø±Ø¯Ø§Ø®Øª Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ',
    'Ù…Ù‡Ù„Øª ØªØ³Ù„ÛŒÙ… Ø§Ø¸Ù‡Ø§Ø±Ù†Ø§Ù…Ù‡ Ù…Ø§Ù„ÛŒØ§ØªÛŒ Ú†Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø§Ø³ØªØŸ',
    'Ø¬Ø±Ø§ÛŒÙ… Ø¹Ø¯Ù… Ù¾Ø±Ø¯Ø§Ø®Øª Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ Ù…Ø§Ù„ÛŒØ§Øª Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ',
    'Ù…Ø§Ù„ÛŒØ§Øª Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ Ú†Ú¯ÙˆÙ†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ',
    
    # Ø¨ÛŒÙ…Ù‡ Ú©Ø§Ø±Ú¯Ø±ÛŒ (7 Ø³ÙˆØ§Ù„)
    'Ø­Ù‚ Ø¨ÛŒÙ…Ù‡ ØªØ§Ù…ÛŒÙ† Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ú†Ú¯ÙˆÙ†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ',
    'Ø³Ù‡Ù… Ú©Ø§Ø±ÙØ±Ù…Ø§ Ùˆ Ú©Ø§Ø±Ú¯Ø± Ø§Ø² Ø­Ù‚ Ø¨ÛŒÙ…Ù‡ Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ',
    'Ø´Ø±Ø§ÛŒØ· Ø¯Ø±ÛŒØ§ÙØª Ø¨ÛŒÙ…Ù‡ Ø¨ÛŒÚ©Ø§Ø±ÛŒ Ú†ÛŒØ³ØªØŸ',
    'Ù…Ø¯Øª Ù¾Ø±Ø¯Ø§Ø®Øª Ø¨ÛŒÙ…Ù‡ Ø¨ÛŒÚ©Ø§Ø±ÛŒ Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ',
    'Ø¨ÛŒÙ…Ù‡ Ø­ÙˆØ§Ø¯Ø« Ú©Ø§Ø± Ú†Ù‡ Ù…ÙˆØ§Ø±Ø¯ÛŒ Ø±Ø§ Ù¾ÙˆØ´Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ØŸ',
    'Ø³Ù†ÙˆØ§Øª Ø¨Ø§Ø²Ù†Ø´Ø³ØªÚ¯ÛŒ Ø¯Ø± ØªØ§Ù…ÛŒÙ† Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ',
    'Ù†Ø­ÙˆÙ‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø³ØªÙ…Ø±ÛŒ Ø¨Ø§Ø²Ù†Ø´Ø³ØªÚ¯ÛŒ Ú†Ú¯ÙˆÙ†Ù‡ Ø§Ø³ØªØŸ',
    
    # Ú¯Ù…Ø±Ú© (6 Ø³ÙˆØ§Ù„)
    'ØªØ¹Ø±ÙÙ‡ Ú¯Ù…Ø±Ú©ÛŒ Ú†ÛŒØ³Øª Ùˆ Ú†Ú¯ÙˆÙ†Ù‡ ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ',
    'Ø­Ù‚ÙˆÙ‚ ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ù„Ø§ Ú†Ú¯ÙˆÙ†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ',
    'Ù…Ø§Ù„ÛŒØ§Øª Ø¨Ø± ÙˆØ§Ø±Ø¯Ø§Øª Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ',
    'Ù…Ø¯Ø§Ø±Ú© Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ ØªØ±Ø®ÛŒØµ Ú©Ø§Ù„Ø§ Ø§Ø² Ú¯Ù…Ø±Ú© Ú†ÛŒØ³ØªØŸ',
    'Ù…Ø¹Ø§ÙÛŒØª Ú¯Ù…Ø±Ú©ÛŒ Ø¯Ø± Ú†Ù‡ Ù…ÙˆØ§Ø±Ø¯ÛŒ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ',
    'Ø¬Ø±ÛŒÙ…Ù‡ Ø¹Ø¯Ù… Ø§Ø¸Ù‡Ø§Ø± Ú©Ø§Ù„Ø§ Ø¯Ø± Ú¯Ù…Ø±Ú© Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ'
]

def create_test_jwt():
    """Ø§ÛŒØ¬Ø§Ø¯ JWT Ø¨Ø±Ø§ÛŒ ØªØ³Øª"""
    from jose import jwt
    
    payload = {
        'sub': 'test_user_timing_analysis',
        'exp': datetime.now(timezone.utc) + timedelta(hours=2),
        'type': 'access'
    }
    
    token = jwt.encode(payload, JWT_SECRET, algorithm='HS256')
    return token

def test_query_with_timing(query: str, token: str, query_num: int) -> Dict[str, Any]:
    """
    ØªØ³Øª ÛŒÚ© Ø³ÙˆØ§Ù„ Ø¨Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø²Ù…Ø§Ù† Ù‡Ø± Ù…Ø±Ø­Ù„Ù‡
    """
    timings = {
        'total_start': time.time(),
        'request_sent': None,
        'response_received': None,
        'total_end': None
    }
    
    try:
        # Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª
        timings['request_sent'] = time.time()
        
        response = httpx.post(
            f'{CORE_API_URL}/api/v1/query',
            headers={
                'Authorization': f'Bearer {token}',
                'Content-Type': 'application/json'
            },
            json={
                'query': query,
                'language': 'fa',
                'enable_web_search': False
            },
            timeout=120.0,
            follow_redirects=True
        )
        
        timings['response_received'] = time.time()
        timings['total_end'] = time.time()
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§
        network_time = (timings['response_received'] - timings['request_sent']) * 1000
        total_time = (timings['total_end'] - timings['total_start']) * 1000
        
        if response.status_code == 200:
            result = response.json()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª timing Ø§Ø² response
            processing_time = result.get('processing_time_ms', 0)
            
            return {
                'success': True,
                'query': query,
                'query_num': query_num,
                
                # Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§ (ms)
                'total_time_ms': total_time,
                'network_time_ms': network_time,
                'processing_time_ms': processing_time,
                'overhead_time_ms': total_time - processing_time,
                
                # Ø§Ø·Ù„Ø§Ø¹Ø§Øª LLM
                'tokens_used': result.get('tokens_used', 0),
                'input_tokens': result.get('input_tokens', 0),
                'output_tokens': result.get('output_tokens', 0),
                
                # Ø§Ø·Ù„Ø§Ø¹Ø§Øª RAG
                'sources_count': len(result.get('sources', [])),
                'context_used': result.get('context_used', False),
                
                # Ù¾Ø§Ø³Ø®
                'answer_length': len(result.get('answer', '')),
                'answer_preview': result.get('answer', '')[:200],
                
                # Ù…ØªØ§Ø¯ÛŒØªØ§
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
        else:
            return {
                'success': False,
                'query': query,
                'query_num': query_num,
                'error': f'HTTP {response.status_code}',
                'total_time_ms': total_time,
                'response_text': response.text[:300],
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            
    except httpx.TimeoutException as e:
        return {
            'success': False,
            'query': query,
            'query_num': query_num,
            'error': 'Timeout',
            'error_detail': str(e)[:200],
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        return {
            'success': False,
            'query': query,
            'query_num': query_num,
            'error': type(e).__name__,
            'error_detail': str(e)[:200],
            'timestamp': datetime.now(timezone.utc).isoformat()
        }

def analyze_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ùˆ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú¯Ù„ÙˆÚ¯Ø§Ù‡â€ŒÙ‡Ø§"""
    successful = [r for r in results if r['success']]
    failed = [r for r in results if not r['success']]
    
    if not successful:
        return {
            'success_rate': 0,
            'total_queries': len(results),
            'failed_count': len(failed),
            'errors': [{'query': r['query'][:50], 'error': r.get('error')} for r in failed]
        }
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§
    total_times = [r['total_time_ms'] for r in successful]
    network_times = [r['network_time_ms'] for r in successful]
    processing_times = [r['processing_time_ms'] for r in successful]
    overhead_times = [r['overhead_time_ms'] for r in successful]
    
    tokens_used = [r['tokens_used'] for r in successful]
    input_tokens = [r['input_tokens'] for r in successful]
    output_tokens = [r['output_tokens'] for r in successful]
    sources = [r['sources_count'] for r in successful]
    
    analysis = {
        # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
        'success_rate': len(successful) / len(results) * 100,
        'total_queries': len(results),
        'successful_count': len(successful),
        'failed_count': len(failed),
        
        # ØªØ­Ù„ÛŒÙ„ Ø²Ù…Ø§Ù†ÛŒ (ms)
        'timing_analysis': {
            'total_time': {
                'avg': sum(total_times) / len(total_times),
                'min': min(total_times),
                'max': max(total_times),
                'median': sorted(total_times)[len(total_times)//2]
            },
            'network_time': {
                'avg': sum(network_times) / len(network_times),
                'min': min(network_times),
                'max': max(network_times),
                'percentage': (sum(network_times) / sum(total_times)) * 100
            },
            'processing_time': {
                'avg': sum(processing_times) / len(processing_times),
                'min': min(processing_times),
                'max': max(processing_times),
                'percentage': (sum(processing_times) / sum(total_times)) * 100
            },
            'overhead_time': {
                'avg': sum(overhead_times) / len(overhead_times),
                'min': min(overhead_times),
                'max': max(overhead_times),
                'percentage': (sum(overhead_times) / sum(total_times)) * 100
            }
        },
        
        # ØªØ­Ù„ÛŒÙ„ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§
        'token_analysis': {
            'total_tokens': {
                'avg': sum(tokens_used) / len(tokens_used),
                'min': min(tokens_used),
                'max': max(tokens_used),
                'total': sum(tokens_used)
            },
            'input_tokens': {
                'avg': sum(input_tokens) / len(input_tokens),
                'total': sum(input_tokens)
            },
            'output_tokens': {
                'avg': sum(output_tokens) / len(output_tokens),
                'total': sum(output_tokens)
            }
        },
        
        # ØªØ­Ù„ÛŒÙ„ RAG
        'rag_analysis': {
            'avg_sources': sum(sources) / len(sources),
            'min_sources': min(sources),
            'max_sources': max(sources),
            'context_usage_rate': sum(1 for r in successful if r['context_used']) / len(successful) * 100
        },
        
        # Ú¯Ù„ÙˆÚ¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
        'bottlenecks': identify_bottlenecks(successful),
        
        # Ø®Ø·Ø§Ù‡Ø§
        'errors': [{'query': r['query'][:50], 'error': r.get('error')} for r in failed] if failed else []
    }
    
    return analysis

def identify_bottlenecks(successful_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú¯Ù„ÙˆÚ¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ"""
    if not successful_results:
        return {}
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‡Ø± Ø¨Ø®Ø´
    avg_network = sum(r['network_time_ms'] for r in successful_results) / len(successful_results)
    avg_processing = sum(r['processing_time_ms'] for r in successful_results) / len(successful_results)
    avg_overhead = sum(r['overhead_time_ms'] for r in successful_results) / len(successful_results)
    avg_total = sum(r['total_time_ms'] for r in successful_results) / len(successful_results)
    
    # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ú¯Ù„ÙˆÚ¯Ø§Ù‡
    components = {
        'network': avg_network,
        'processing': avg_processing,
        'overhead': avg_overhead
    }
    
    main_bottleneck = max(components, key=components.get)
    
    return {
        'main_bottleneck': main_bottleneck,
        'main_bottleneck_time_ms': components[main_bottleneck],
        'main_bottleneck_percentage': (components[main_bottleneck] / avg_total) * 100,
        'breakdown': {
            'network_ms': avg_network,
            'processing_ms': avg_processing,
            'overhead_ms': avg_overhead,
            'total_ms': avg_total
        },
        'recommendations': generate_recommendations(main_bottleneck, components)
    }

def generate_recommendations(bottleneck: str, components: Dict[str, float]) -> List[str]:
    """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú¯Ù„ÙˆÚ¯Ø§Ù‡"""
    recommendations = []
    
    if bottleneck == 'processing':
        recommendations.extend([
            'Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ LLM: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø³Ø±ÛŒØ¹ØªØ± (Ù…Ø«Ù„Ø§Ù‹ gpt-4o-mini Ø¨Ù‡ Ø¬Ø§ÛŒ gpt-4o)',
            'Ú©Ø§Ù‡Ø´ context window: Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ chunks Ø§Ø±Ø³Ø§Ù„ÛŒ Ø¨Ù‡ LLM',
            'Ø¨Ø±Ø±Ø³ÛŒ reranker: Ø²Ù…Ø§Ù† reranking Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø²ÛŒØ§Ø¯ Ø¨Ø§Ø´Ø¯',
            'Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² caching Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒ'
        ])
    elif bottleneck == 'network':
        recommendations.extend([
            'Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ Ø´Ø¨Ú©Ù‡ Ø¨Ù‡ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ (OpenAI/GapGPT)',
            'Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CDN ÛŒØ§ proxy Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±',
            'Ø§ÙØ²Ø§ÛŒØ´ timeout Ùˆ retry mechanism',
            'Ø¨Ø±Ø±Ø³ÛŒ latency Ø¨Ù‡ Ø³Ø±ÙˆØ± LLM'
        ])
    elif bottleneck == 'overhead':
        recommendations.extend([
            'Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Qdrant query: Ú©Ø§Ù‡Ø´ Ø²Ù…Ø§Ù† Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± vector DB',
            'Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ embedding: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø³Ø±ÛŒØ¹ØªØ± ÛŒØ§ local embedding',
            'Ú©Ø§Ù‡Ø´ overhead Ø³Ø±ÛŒØ§Ù„ÛŒØ²Ø§Ø³ÛŒÙˆÙ† Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡',
            'Ø¨Ø±Ø±Ø³ÛŒ performance database queries'
        ])
    
    return recommendations

def print_summary(analysis: Dict[str, Any], config: Dict[str, str]):
    """Ú†Ø§Ù¾ Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬"""
    print("\n" + "=" * 80)
    print("ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ… RAG")
    print("=" * 80)
    
    print(f"\nğŸ¯ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ¹Ù„ÛŒ:")
    print(f"   LLM2 Primary: {config['llm2_model']} ({config['llm2_provider']})")
    print(f"   LLM2 Fallback: {config['llm2_fallback_model']} ({config['llm2_fallback_provider']})")
    print(f"   Reranker: {config['reranker_url']}")
    
    print(f"\nâœ… Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª: {analysis['success_rate']:.1f}% ({analysis['successful_count']}/{analysis['total_queries']})")
    
    if analysis['success_rate'] > 0:
        timing = analysis['timing_analysis']
        
        print(f"\nâ±ï¸  ØªØ­Ù„ÛŒÙ„ Ø²Ù…Ø§Ù†ÛŒ (Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†):")
        print(f"   Ú©Ù„ Ø²Ù…Ø§Ù†: {timing['total_time']['avg']:.0f}ms")
        print(f"   â”œâ”€ Ø´Ø¨Ú©Ù‡: {timing['network_time']['avg']:.0f}ms ({timing['network_time']['percentage']:.1f}%)")
        print(f"   â”œâ”€ Ù¾Ø±Ø¯Ø§Ø²Ø´: {timing['processing_time']['avg']:.0f}ms ({timing['processing_time']['percentage']:.1f}%)")
        print(f"   â””â”€ overhead: {timing['overhead_time']['avg']:.0f}ms ({timing['overhead_time']['percentage']:.1f}%)")
        
        print(f"\n   Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø²Ù…Ø§Ù†ÛŒ: {timing['total_time']['min']:.0f}ms - {timing['total_time']['max']:.0f}ms")
        print(f"   Ù…ÛŒØ§Ù†Ù‡: {timing['total_time']['median']:.0f}ms")
        
        tokens = analysis['token_analysis']
        print(f"\nğŸ« Ù…ØµØ±Ù ØªÙˆÚ©Ù†:")
        print(f"   Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†: {tokens['total_tokens']['avg']:.0f} (ÙˆØ±ÙˆØ¯ÛŒ: {tokens['input_tokens']['avg']:.0f}, Ø®Ø±ÙˆØ¬ÛŒ: {tokens['output_tokens']['avg']:.0f})")
        print(f"   Ú©Ù„: {tokens['total_tokens']['total']:.0f} ØªÙˆÚ©Ù†")
        
        rag = analysis['rag_analysis']
        print(f"\nğŸ“š Ø¹Ù…Ù„Ú©Ø±Ø¯ RAG:")
        print(f"   Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ù†Ø§Ø¨Ø¹: {rag['avg_sources']:.1f}")
        print(f"   Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² context: {rag['context_usage_rate']:.1f}%")
        
        bottlenecks = analysis['bottlenecks']
        print(f"\nğŸš¨ Ú¯Ù„ÙˆÚ¯Ø§Ù‡ Ø§ØµÙ„ÛŒ: {bottlenecks['main_bottleneck'].upper()}")
        print(f"   Ø²Ù…Ø§Ù†: {bottlenecks['main_bottleneck_time_ms']:.0f}ms ({bottlenecks['main_bottleneck_percentage']:.1f}%)")
        
        print(f"\nğŸ’¡ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ:")
        for i, rec in enumerate(bottlenecks['recommendations'], 1):
            print(f"   {i}. {rec}")
    
    if analysis['failed_count'] > 0:
        print(f"\nâŒ Ø®Ø·Ø§Ù‡Ø§ ({analysis['failed_count']}):")
        for err in analysis['errors'][:5]:
            print(f"   - {err['query']}: {err['error']}")

def main():
    print("=" * 80)
    print("ğŸ”¬ ØªØ³Øª Ø¬Ø§Ù…Ø¹ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ… RAG - ØªØ­Ù„ÛŒÙ„ Ø²Ù…Ø§Ù†ÛŒ Ø¯Ù‚ÛŒÙ‚")
    print("=" * 80)
    print(f"ØªØ¹Ø¯Ø§Ø¯ Ø³ÙˆØ§Ù„Ø§Øª: {len(BUSINESS_QUERIES)}")
    print(f"Ø´Ø±ÙˆØ¹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    config = {
        'llm2_model': settings.llm2_model,
        'llm2_provider': 'GapGPT' if 'gapgpt' in settings.llm2_base_url.lower() else 'OpenAI',
        'llm2_fallback_model': settings.llm2_fallback_model,
        'llm2_fallback_provider': 'OpenAI' if 'openai' in settings.llm2_fallback_base_url.lower() else 'Unknown',
        'reranker_url': settings.reranker_service_url
    }
    
    # Ø§ÛŒØ¬Ø§Ø¯ JWT
    token = create_test_jwt()
    print(f"\nâœ… JWT Token Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
    
    # Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§
    results = []
    print(f"\nğŸš€ Ø´Ø±ÙˆØ¹ ØªØ³Øª {len(BUSINESS_QUERIES)} Ø³ÙˆØ§Ù„...\n")
    
    for i, query in enumerate(BUSINESS_QUERIES, 1):
        print(f"[{i:2d}/{len(BUSINESS_QUERIES)}] {query[:60]}...", end=' ', flush=True)
        
        result = test_query_with_timing(query, token, i)
        results.append(result)
        
        if result['success']:
            print(f"âœ… {result['total_time_ms']:.0f}ms", flush=True)
        else:
            print(f"âŒ {result.get('error', 'Unknown')}", flush=True)
        
        # Ø§Ø³ØªØ±Ø§Ø­Øª Ú©ÙˆØªØ§Ù‡
        if i < len(BUSINESS_QUERIES):
            time.sleep(0.5)
    
    # ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬
    analysis = analyze_results(results)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    output_data = {
        'config': config,
        'timestamp': datetime.now(timezone.utc).isoformat(),
        'queries_count': len(BUSINESS_QUERIES),
        'results': results,
        'analysis': analysis
    }
    
    output_file = '/tmp/business_timing_analysis.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    # Ú†Ø§Ù¾ Ø®Ù„Ø§ØµÙ‡
    print_summary(analysis, config)
    
    print(f"\nğŸ’¾ Ù†ØªØ§ÛŒØ¬ Ú©Ø§Ù…Ù„ Ø¯Ø± {output_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    print("\nğŸ“– Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª:")
    print(f"   cat {output_file} | jq '.analysis'")
    print(f"   cat {output_file} | jq '.results[] | select(.success==false)'")
    
    return 0 if analysis['success_rate'] >= 80 else 1

if __name__ == '__main__':
    exit(main())
